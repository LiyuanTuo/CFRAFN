{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 备用代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"{name}:{param.shape}\")\n",
    "\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 检查是否存在重复的文件名\n",
    "# import os\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # 指定文件夹路径\n",
    "# folder_path = r'../data/'\n",
    "\n",
    "# # 使用字典来存储文件名及其出现次数\n",
    "# file_count = defaultdict(int)\n",
    "\n",
    "# # 使用 os.walk 遍历文件夹及其子文件夹中的所有文件\n",
    "# for root, dirs, files in os.walk(folder_path):\n",
    "    \n",
    "#     for filename in files:\n",
    "#         print(filename)\n",
    "#         file_count[filename] += 1\n",
    "\n",
    "# # 检查是否有重复的文件名\n",
    "# duplicates = {filename: count for filename, count in file_count.items() if count > 1}\n",
    "\n",
    "# if duplicates:\n",
    "#     print(\"发现重复的文件名：\")\n",
    "#     for filename, count in duplicates.items():\n",
    "#         print(f\"{filename}: {count} 次\")\n",
    "# else:\n",
    "#     print(\"没有发现重复的文件名。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_vggish_features(file_path, sampling_rate=None):\n",
    "#     \"\"\"\n",
    "#     提取单个音频文件的 VGGish 特征。\n",
    "    \n",
    "#     参数：\n",
    "#     - file_path (str): 音频文件的路径。\n",
    "#     - sampling_rate (int, optional): 采样率。默认为 None，表示从文件中读取。\n",
    "    \n",
    "#     返回值：\n",
    "#     - features_vggish (torch.Tensor): 提取的特征张量。\n",
    "#     \"\"\"\n",
    "#     model = vggish()\n",
    "#     input_batch = vggish_input.wavfile_to_examples(file_path)\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         features_vggish = model(input_batch)\n",
    "#         features_vggish = torch.mean(features_vggish, dim=0, keepdim=True)\n",
    "#     return features_vggish\n",
    "\n",
    "# def vggish_enhance_data(file, label):\n",
    "#     \"\"\"\n",
    "#     对音频数据进行增强处理。\n",
    "    \n",
    "#     参数：\n",
    "#     - file (str): 音频文件的路径。\n",
    "#     - label (int): 音频的标签。\n",
    "    \n",
    "#     返回值：\n",
    "#     - group (list): 增强后的音频数据列表。\n",
    "#     - sampling_rate (int): 采样率。\n",
    "#     - label (int): 标签。\n",
    "#     \"\"\"\n",
    "#     data, sampling_rate = librosa.load(file, sr=None)\n",
    "#     noises = data + 0.05 * np.random.randn(len(data))  # 添加噪声\n",
    "#     pitches = librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=2)  # 改变音高\n",
    "#     stretches = librosa.effects.time_stretch(data, rate=2)  # 改变速度\n",
    "#     volumes = data * 2  # 调整音量\n",
    "#     mid_index = len(data) // 2\n",
    "#     cut1 = data[:mid_index]\n",
    "#     cut2 = data[mid_index:]\n",
    "\n",
    "#     group = [data, noises, pitches, stretches, volumes, cut1, cut2]\n",
    "#     return group, sampling_rate, label\n",
    "\n",
    "# def process_vggish_group(group, sampling_rate, label):\n",
    "#     \"\"\"\n",
    "#     处理一组音频数据，提取 VGGish 特征。\n",
    "    \n",
    "#     参数：\n",
    "#     - group (list): 音频数据组。\n",
    "#     - sampling_rate (int): 采样率。\n",
    "#     - label (int): 标签。\n",
    "    \n",
    "#     返回值：\n",
    "#     - enhance (list): 提取的特征张量列表。\n",
    "#     \"\"\"\n",
    "#     enhance = []\n",
    "#     for g in group:\n",
    "#         features_vggish = extract_vggish_features(g, sampling_rate)\n",
    "#         features_vggish = features_vggish.mean(dim=0, keepdim=True)\n",
    "#         df_vggish = pd.DataFrame(features_vggish.numpy(), columns=[f'feature_{i}' for i in range(features_vggish.shape[1])])\n",
    "#         df_vggish['label'] = label\n",
    "#         enhance.append(df_vggish)\n",
    "#     return enhance\n",
    "\n",
    "# def get_vggish_features(data_index, train=False, n_jobs=-1):\n",
    "#     features_df = []\n",
    "#     def process_file(index):\n",
    "#         filename = X[index]\n",
    "#         label = y[index]\n",
    "#         found = False\n",
    "#         for root, dirs, files in os.walk(self_folder):\n",
    "#             if filename in files:\n",
    "#                 full_path = os.path.join(root, filename)\n",
    "#                 if train:\n",
    "#                     group, sampling_rate, label = vggish_enhance_data(full_path, label)\n",
    "#                     features_vggish = process_vggish_group(group, sampling_rate, label)\n",
    "#                     return features_vggish\n",
    "#                 else:\n",
    "#                     model = vggish()\n",
    "#                     input_batch = vggish_input.wavfile_to_examples(full_path)\n",
    "#                     with torch.no_grad():\n",
    "#                         model.eval()\n",
    "#                         features_vggish = model(input_batch)\n",
    "#                         features_vggish = features_vggish.mean(dim=0, keepdim=True)\n",
    "#                         df_vggish = pd.DataFrame(features_vggish.numpy(), columns=[f'feature_{i}' for i in range(features_vggish.shape[1])])\n",
    "#                         df_vggish['label'] = label\n",
    "#                     return [df_vggish]\n",
    "#         if not found:\n",
    "#             print(f'File not found.')\n",
    "#         return []\n",
    "    \n",
    "#     results = Parallel(n_jobs=n_jobs)(delayed(process_file)(index) for index in tqdm(data_index))\n",
    "#     features_df = [item for sublist in results for item in sublist]\n",
    "#     combined_df = pd.concat(features_df, ignore_index=True)\n",
    "#     x_df = combined_df.drop(columns=['label'])\n",
    "#     y_df = combined_df['label']\n",
    "#     return x_df, y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # next  features_vggish\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torchvggish import vggish, vggish_input\n",
    "# from joblib import Parallel, delayed\n",
    "# def extract_vggish_features(file_path, sampling_rate=None):\n",
    "#     \"\"\"\n",
    "#     提取单个音频文件的 VGGish 特征。\n",
    "    \n",
    "#     参数：\n",
    "#     - file_path (str): 音频文件的路径。\n",
    "#     - sampling_rate (int, optional): 采样率。默认为 None，表示从文件中读取。\n",
    "    \n",
    "#     返回值：\n",
    "#     - features_vggish (torch.Tensor): 提取的特征张量。\n",
    "#     \"\"\"\n",
    "#     model = vggish()\n",
    "#     input_batch = vggish_input.wavfile_to_examples(file_path)\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         features_vggish = model(input_batch)\n",
    "#         features_vggish = torch.mean(features_vggish, dim=0, keepdim=True)\n",
    "#     return features_vggish\n",
    "\n",
    "# def vggish_enhance_data(file, label):\n",
    "#     \"\"\"\n",
    "#     对音频数据进行增强处理。\n",
    "    \n",
    "#     参数：\n",
    "#     - file (str): 音频文件的路径。\n",
    "#     - label (int): 音频的标签。\n",
    "    \n",
    "#     返回值：\n",
    "#     - group (list): 增强后的音频数据列表。\n",
    "#     - sampling_rate (int): 采样率。\n",
    "#     - label (int): 标签。\n",
    "#     \"\"\"\n",
    "#     data, sampling_rate = librosa.load(file, sr=None)\n",
    "#     noises = data + 0.05 * np.random.randn(len(data))  # 添加噪声\n",
    "#     pitches = librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=2)  # 改变音高\n",
    "#     stretches = librosa.effects.time_stretch(data, rate=2)  # 改变速度\n",
    "#     volumes = data * 2  # 调整音量\n",
    "#     mid_index = len(data) // 2\n",
    "#     cut1 = data[:mid_index]\n",
    "#     cut2 = data[mid_index:]\n",
    "\n",
    "#     group = [data, noises, pitches, stretches, volumes, cut1, cut2]\n",
    "#     return group, sampling_rate, label\n",
    "\n",
    "# def process_vggish_group(group, sampling_rate, label):\n",
    "#     \"\"\"\n",
    "#     处理一组音频数据，提取 VGGish 特征。\n",
    "    \n",
    "#     参数：\n",
    "#     - group (list): 音频数据组。\n",
    "#     - sampling_rate (int): 采样率。\n",
    "#     - label (int): 标签。\n",
    "    \n",
    "#     返回值：\n",
    "#     - enhance (list): 提取的特征张量列表。\n",
    "#     \"\"\"\n",
    "#     enhance = []\n",
    "#     for g in group:\n",
    "#         features_vggish = extract_vggish_features(g, sampling_rate)\n",
    "#         features_vggish = features_vggish.mean(dim=0, keepdim=True)\n",
    "#         df_vggish = pd.DataFrame(features_vggish.numpy(), columns=[f'feature_{i}' for i in range(features_vggish.shape[1])])\n",
    "#         df_vggish['label'] = label\n",
    "#         enhance.append(df_vggish)\n",
    "#     return enhance\n",
    "\n",
    "# def get_vggish_features(data_index, train=False, n_jobs=-1):\n",
    "#     features_df = []\n",
    "#     def process_file(index):\n",
    "#         filename = X[index]\n",
    "#         label = y[index]\n",
    "#         found = False\n",
    "#         for root, dirs, files in os.walk(self_folder):\n",
    "#             if filename in files:\n",
    "#                 full_path = os.path.join(root, filename)\n",
    "#                 if train:\n",
    "#                     group, sampling_rate, label = vggish_enhance_data(full_path, label)\n",
    "#                     features_vggish = process_vggish_group(group, sampling_rate, label)\n",
    "#                     return features_vggish\n",
    "#                 else:\n",
    "#                     model = vggish()\n",
    "#                     input_batch = vggish_input.wavfile_to_examples(full_path)\n",
    "#                     with torch.no_grad():\n",
    "#                         model.eval()\n",
    "#                         features_vggish = model(input_batch)\n",
    "#                         features_vggish = features_vggish.mean(dim=0, keepdim=True)\n",
    "#                         df_vggish = pd.DataFrame(features_vggish.numpy(), columns=[f'feature_{i}' for i in range(features_vggish.shape[1])])\n",
    "#                         df_vggish['label'] = label\n",
    "#                     return [df_vggish]\n",
    "#         if not found:\n",
    "#             print(f'File not found.')\n",
    "#         return []\n",
    "    \n",
    "#     results = Parallel(n_jobs=n_jobs)(delayed(process_file)(index) for index in tqdm(data_index))\n",
    "#     features_df = [item for sublist in results for item in sublist]\n",
    "#     features_df = pd.concat(features_df).reset_index(drop=True)\n",
    "\n",
    "#     # 去除方差为0的列\n",
    "#     features_df = features_df.loc[:, (features_df.var() != 0)]\n",
    "    \n",
    "#     # 对特征进行归一化\n",
    "#     features = features_df.drop(columns=['label'])\n",
    "#     scaler = StandardScaler()\n",
    "#     features_scaled = scaler.fit_transform(features)\n",
    "#     features_df_scaled = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "#     # 标签组合\n",
    "#     features_df_scaled[\"label\"] = features_df['label']\n",
    "\n",
    "#     x_df = features_df_scaled.drop(columns=['label'])\n",
    "#     y_df = features_df_scaled['label']\n",
    "#     return x_df, y_df, x_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过去计算重要性的公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_zero_min = feature_importances_df[feature_importances_df > 0].min().min()\n",
    "# weights = 1 / (feature_importances_df.replace(0, np.nan).min())\n",
    "# weighted_mean = (feature_importances_df * weights).sum(axis=1) / weights.sum()\n",
    "\n",
    "# 重新排序加权平均值\n",
    "# sorted_indices = weighted_mean.argsort()[::-1]\n",
    "# sorted_weighted_mean = weighted_mean.iloc[sorted_indices]\n",
    "# sorted_weighted_mean.to_csv(r'../result/02sorted_feature_importance_scaled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vgg特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_vggish_features(file_path, sampling_rate=None):\n",
    "#     model = vggish()\n",
    "#     input_batch = vggish_input.waveform_to_examples(file_path, sampling_rate) # input_batch size = torch.Size([45, 1, 96, 64])\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         features_vggish = model(input_batch) # features_vggish=torch.Size([45, 128])\n",
    "#     return features_vggish \n",
    "\n",
    "# def vggish_enhance_data(file, label):\n",
    "#     data, sampling_rate = librosa.load(file, sr=None) \n",
    "#     noises = data + 0.05 * np.random.randn(len(data)) # 向音频数据中添加不同强度的随机噪声\n",
    "#     pitches = librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=2) # 改变音频的音高\n",
    "#     stretches = librosa.effects.time_stretch(data, rate=2) # 改变音频的播放速度\n",
    "#     volumes = data * 2 # 调整音频的音量\n",
    "#     # 将音频数据从中间切割为两部分\n",
    "#     mid_index = len(data) // 2\n",
    "#     cut1 = data[:mid_index]\n",
    "#     cut2 = data[mid_index:]\n",
    "\n",
    "#     group = [data,noises,pitches,stretches,volumes,cut1,cut2]\n",
    "#     return group, sampling_rate, label\n",
    "\n",
    "# def process_vggish_group(group, sampling_rate, label):\n",
    "#     enhance = []\n",
    "#     for g in group:\n",
    "#         features_vggish = extract_vggish_features(g, sampling_rate)\n",
    "#         features_vggish = features_vggish.mean(dim=0, keepdim=True) # 平均池化\n",
    "#         df_vggish = pd.DataFrame(features_vggish.numpy(), columns=[f'feature_{i}' for i in range(features_vggish.shape[1])])\n",
    "#         df_vggish['label'] = label\n",
    "#         enhance.append(df_vggish)\n",
    "#     return enhance\n",
    "\n",
    "# def get_vggish_features(data_index, train=False, n_jobs=-1):\n",
    "#     features_df = []\n",
    "#     def process_file(index):\n",
    "#         filename = X[index]\n",
    "#         label = y[index]\n",
    "#         found = False\n",
    "#         for root, dirs, files in os.walk(self_folder):\n",
    "#             if filename in files:\n",
    "#                 full_path = os.path.join(root, filename)\n",
    "#                 if train:\n",
    "#                     group, sampling_rate, label = vggish_enhance_data(full_path, label)\n",
    "#                     features_vggish = process_vggish_group(group, sampling_rate, label)\n",
    "#                     return features_vggish\n",
    "#                 else:\n",
    "#                     model = vggish()\n",
    "#                     input_batch = vggish_input.wavfile_to_examples(full_path)\n",
    "#                     with torch.no_grad():\n",
    "#                         model.eval()\n",
    "#                         features_vggish = model(input_batch)\n",
    "#                         features_vggish = features_vggish.mean(dim=0, keepdim=True)\n",
    "#                         df_vggish = pd.DataFrame(features_vggish.numpy(), columns=[f'feature_{i}'for i in range(features_vggish.shape[1])] )\n",
    "#                         df_vggish['label'] = label\n",
    "#                     return [df_vggish]\n",
    "#         if not found:\n",
    "#             print(f'File not found.')\n",
    "#         return []\n",
    "#     results = Parallel(n_jobs=n_jobs)(delayed(process_file)(index) for index in data_index)\n",
    "#     features_df = [item for sublist in results for item in sublist]\n",
    "#     features_df = pd.concat(features_df, ignore_index=True)\n",
    "#     # 去除方差为0的列\n",
    "#     features_df = features_df.loc[:, (features_df.var() != 0)]\n",
    "    \n",
    "#     # 对特征进行归一化\n",
    "#     features = features_df.drop(columns=['label'])   \n",
    "#     scaler = StandardScaler()\n",
    "#     features_scaled = scaler.fit_transform(features)\n",
    "#     features_df_scaled = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "#     # 标签组合\n",
    "#     features_df_scaled[\"label\"] = features_df['label']\n",
    "\n",
    "#     x_df = features_df_scaled.drop(columns=['label'])\n",
    "#     y_df = features_df_scaled['label']\n",
    "#     return x_df, y_df, x_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=fold, shuffle=True, random_state=42)\n",
    "# # 重复实验次数\n",
    "# for _ in range(1):\n",
    "#     f1_score_list = []\n",
    "    \n",
    "#     for train_index, test_index in skf.split(X, y):\n",
    "#         setup_seed(42)\n",
    "#         # 获取 eGeMAPS 特征\n",
    "#         X_train_eGe, y_train = get_eGe_matrix(train_index, train=True, n_jobs=-1)\n",
    "#         X_test_eGe, y_test = get_eGe_matrix(test_index, n_jobs=-1)\n",
    "\n",
    "#         # 获取 VGGish 特征\n",
    "#         X_train_VGGish, _ = get_vggish_features(train_index, train=True, n_jobs=-1)\n",
    "#         X_test_VGGish, _ = get_vggish_features(test_index, n_jobs=-1)\n",
    "\n",
    "#         # X-train, X-test\n",
    "#         X_train = (X_train_eGe, X_train_VGGish)\n",
    "#         X_test = (X_test_eGe, X_test_VGGish)\n",
    "        \n",
    "#         model = EGV_AttNet(input_dim_eGeMAPS=88, input_dim_VGGish=128, \n",
    "#                            num_classes=2, feature_weights= feature_weights, num_conv_layers=2, \n",
    "#                            conv_out_channels=128).to(device)\n",
    "#         optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "#         criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "#         #训练并评估模型\n",
    "#         f1 = train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test)\n",
    "#         f1_score_list.append(f1)\n",
    "#     print(f\"Mean f1_score: {np.mean(f1_score_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模拟并行顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# def process_file(index):\n",
    "#     # 模拟处理逻辑，返回处理结果\n",
    "#     return f\"processed_{index}\"\n",
    "\n",
    "# data_index = [i for i in range(1000)]\n",
    "# n_jobs = -1  # 并行任务数\n",
    "\n",
    "# results = Parallel(n_jobs=n_jobs)(delayed(process_file)(index) for index in data_index)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习模型结果比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {\n",
    "#     \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "#     \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "#     \"Support Vector Machine\": SVC(probability=True),\n",
    "#     \"Random Forest\": RandomForestClassifier(),\n",
    "#     \"Bagging\": BaggingClassifier(),\n",
    "#     \"AdaBoost\": AdaBoostClassifier(algorithm=\"SAMME\"),\n",
    "#     \"XGBoost\": XGBClassifier(),\n",
    "#     \"LightGBM\": LGBMClassifier(verbosity=-1),\n",
    "#     \"Naive Bayes\": GaussianNB(),\n",
    "#     \"Decision Tree\": DecisionTreeClassifier(),\n",
    "#     \"GBDT\": GradientBoostingClassifier()\n",
    "# }\n",
    "\n",
    "# for name, model in models.items():\n",
    "#     print(f\"Training {name}...\")\n",
    "#     # 建立管道\n",
    "#     pipeline = Pipeline(steps=[('scaler', StandardScaler()), ('classifier', model)])\n",
    "\n",
    "#     evaluate_dics_ml_list = []\n",
    "\n",
    "#     for i in range(len(X_train_eGe_list)):\n",
    "#         setup_seed(42)\n",
    "#         # 获取预先提取的特征\n",
    "#         X_train_eGe = X_train_eGe_list[i]\n",
    "#         X_test_eGe = X_test_eGe_list[i]\n",
    "#         X_train_VGGish = X_train_VGGish_list[i]\n",
    "#         X_test_VGGish = X_test_VGGish_list[i]\n",
    "#         y_train = y_train_list[i]\n",
    "#         y_test = y_test_list[i]\n",
    "\n",
    "#         # 拼接特征\n",
    "#         X_train = np.hstack((X_train_eGe, X_train_VGGish))\n",
    "#         X_test = np.hstack((X_test_eGe, X_test_VGGish))\n",
    "\n",
    "#         # 训练模型\n",
    "#         pipeline.fit(X_train, y_train)\n",
    "\n",
    "#         # 预测\n",
    "#         y_pred = pipeline.predict(X_test)\n",
    "#         y_pred_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#         # 评估\n",
    "#         evaluate_dics_ml = evaluate_model(y_test, y_pred, y_pred_prob)\n",
    "#         evaluate_dics_ml_list.append(evaluate_dics_ml)\n",
    "\n",
    "#     print(f\"Mean F1_score of {name}: {np.mean([evaluate_dics_ml['f1'] for evaluate_dics_ml in evaluate_dics_ml_list])}\")\n",
    "#     print(f\"Mean Accuracy of {name}: {np.mean([evaluate_dics_ml['acc'] for evaluate_dics_ml in evaluate_dics_ml_list])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 feature importance\n",
    "\n",
    "使用集群跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  3  4  6  8  9 10 11 13 14 15 16 17 18 20 21 22 23 25 26 28 29 30\n",
      " 31 32 33 34 35 36 37 40 41 43 44 45 48 49 50 51 52 53 54 55 56 57 58 59]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 19 20 21 22 23 24 25 26\n",
      " 27 28 30 31 32 33 35 37 38 39 40 42 46 47 48 49 50 52 53 54 56 57 58 59]\n",
      "[ 0  2  5  7  9 10 11 12 14 16 17 18 19 20 22 23 24 27 28 29 30 31 32 33\n",
      " 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 55 56 58 59]\n",
      "[ 1  2  3  4  5  6  7  8  9 11 12 13 15 16 17 18 19 20 21 22 24 25 26 27\n",
      " 28 29 31 32 33 34 36 38 39 40 41 42 43 44 45 46 47 51 53 54 55 56 57 59]\n",
      "[ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 17 18 19 21 23 24 25 26 27\n",
      " 29 30 34 35 36 37 38 39 41 42 43 44 45 46 47 48 49 50 51 52 54 55 57 58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import opensmile\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "from tools.common import setup_seed\n",
    "import librosa\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tools.utils import get_eGe_matrix\n",
    "\n",
    "def get_data_csv(root_folder):\n",
    "    cls_files = []\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for filename in files:\n",
    "            cls_files.append((filename, os.path.basename(root).split(\"_\")[-1]))\n",
    "\n",
    "    cls_files_df = pd.DataFrame(cls_files, columns=['name', 'class'])\n",
    "    cls_files_df.to_csv(os.path.join(os.path.dirname(root_folder),os.path.basename(root_folder)+\".csv\"),index=False,encoding=\"utf-8-sig\")\n",
    "\n",
    "root_folder = '../data/group_control'\n",
    "get_data_csv(root_folder)\n",
    "\n",
    "root_folder = '../data/self_control'\n",
    "get_data_csv(root_folder)\n",
    "\n",
    "fold = 5\n",
    "df = pd.read_csv(\"../data/self_control.csv\")\n",
    "setup_seed(111)\n",
    "shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = shuffled_df['name']\n",
    "y = shuffled_df['class']\n",
    "self_folder = \"../data/self_control/\"\n",
    "\n",
    "\n",
    "# 创建一个Smile对象，配置为使用eGeMAPS特征集\n",
    "smile = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "    feature_level=opensmile.FeatureLevel.Functionals,\n",
    ")\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "}\n",
    "\n",
    "# # 去除方差小于等于0.01的特征\n",
    "# with open(\"../result/01preprocess/varLessthan0.01.txt\",'r')as f:\n",
    "#     drop_lst = [line.strip() for line in f]\n",
    "results = {}\n",
    "feature_importances = {clf_name: None for clf_name in classifiers.keys()}\n",
    "clf_f1 = {clf_name: [] for clf_name in classifiers.keys()}\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=fold, shuffle=True, random_state=111)\n",
    "\n",
    "for _ in tqdm(range(1)):\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        print(train_index)\n",
    "        # print(y[train_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征重要性加权计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest        0.290362\n",
      "AdaBoost             0.200100\n",
      "XGBoost              0.282338\n",
      "Gradient Boosting    0.227200\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline\n",
    "# 加载特征重要性数据\n",
    "feature_importances_df = pd.read_csv('../result/01preprocess/02feature_importance.csv',index_col=0)\n",
    "importances_f1_weighted = pd.read_csv('../result/01preprocess/02feature_importance_f1_weighted.csv')\n",
    "\n",
    "# 计算每个分类器的平均预测值\n",
    "classifier_means = importances_f1_weighted.mean(axis=0)\n",
    "\n",
    "# 归一化分类器的平均预测值，得到权重\n",
    "classifier_weights = classifier_means / classifier_means.sum()\n",
    "print(classifier_weights)\n",
    "# 对每个分类器的特征重要性进行归一化（按列归一化）\n",
    "normalized_feature_importances = feature_importances_df.apply(\n",
    "    lambda x: x / x.sum(), axis=0\n",
    ")\n",
    "\n",
    "# 加权归一化后的特征重要性（按分类器权重加权）\n",
    "weighted_feature_importances = normalized_feature_importances * classifier_weights\n",
    "\n",
    "# 最终加权后的特征重要性（按行求和，得到每个特征的总重要性）\n",
    "final_feature_importances = weighted_feature_importances.sum(axis=1) # final_feature_importances.sum() = 1\n",
    "\n",
    "sorted_feature_importances = final_feature_importances.sort_values(ascending=False)\n",
    "sorted_feature_importances.to_csv('../result/01preprocess/03sorted_feature_importance.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算累计贡献比例\n",
    "cumulative_importance = sorted_feature_importances.cumsum() / sorted_feature_importances.sum()\n",
    "\n",
    "# 找到第一个达到或超过99%的索引位置\n",
    "threshold_idx = (cumulative_importance >= 0.95).idxmax()\n",
    "\n",
    "# 提取前N个特征（累计贡献≥99%）\n",
    "selected_features = sorted_feature_importances.loc[:threshold_idx]\n",
    "\n",
    "sorted_feature_importances = selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 去除方差小于等于0.01的特征\n",
    "with open(\"../result/01preprocess/eGe_feature_cumul0.95.txt\",'w')as f:\n",
    "    for i in sorted_feature_importances.index:\n",
    "        f.write(f\"{i}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重要性曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画图\n",
    "x = np.arange(len(sorted_feature_importances))\n",
    "y = sorted_feature_importances.values\n",
    "\n",
    "x_new = np.linspace(x.min(), x.max(), 300)\n",
    "spl = make_interp_spline(x, y, k=1)\n",
    "y_smooth = spl(x_new)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.plot(x_new, y_smooth, color='royalblue', linewidth=2, label='Weighted Feature Importance')\n",
    "plt.bar(x, y, color = 'lightgreen', label = 'Weighted Importance Score', alpha=0.6)\n",
    "\n",
    "ax = plt.gca()  # 获取当前轴\n",
    "ax.spines['top'].set_visible(False)  # 隐藏上边框\n",
    "ax.spines['right'].set_visible(False)  # 隐藏右边框\n",
    "\n",
    "plt.xlabel('Feature Index (Sorted by Importance)')\n",
    "plt.ylabel('Weighted Importance Score')\n",
    "plt.legend(loc=\"upper right\", prop={'size':10}, labelcolor='black')\n",
    "plt.savefig('../result/01preprocess/03Weighted_Feature_Importance_curve_scaled.tif', dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig('../result/01preprocess/03Weighted_Feature_Importance_curve_scaled.pdf', dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# ax.spines['bottom'].set_linewidth(2)  # 加粗X轴\n",
    "# ax.spines['left'].set_linewidth(2)  # 加粗Y轴\n",
    "# 加粗坐标轴刻度\n",
    "# ax.xaxis.set_tick_params(width=2)  # X轴刻度加粗\n",
    "# ax.yaxis.set_tick_params(width=2)  # Y轴刻度加粗\n",
    "# 设置刻度标签的字体\n",
    "# for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "#     label.set_fontweight('bold')\n",
    "\n",
    "# plt.title('Smoothed and Weighted Feature Importance Across ML Algorithms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相关性热图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# 标签列相关性最强的前top_n个特征热图绘制\n",
    "def plot_top_features_heatmap(df, label_column='label', top_n=20):\n",
    "    corr_matrix = df.corr()\n",
    "    # abs_corr_with_label = abs(corr_matrix[label_column]).sort_values(ascending=False)\n",
    "    # top_features = abs_corr_with_label[1:top_n + 1].index.tolist()\n",
    "    # top_corr_matrix = df[top_features + [label_column]].corr()\n",
    "    top_corr_matrix = corr_matrix\n",
    "\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    sns.heatmap(top_corr_matrix, cmap='coolwarm',\n",
    "                cbar_kws={'shrink': .5},\n",
    "                annot=True,\n",
    "                fmt='.2f',\n",
    "                linewidths=.5, square=True)\n",
    "    # plt.title('Top Features Correlation with Label', fontsize=18)\n",
    "\n",
    "    plt.xticks(rotation=45, fontsize=10, ha='right')\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.subplots_adjust(bottom=0.3)\n",
    "    plt.savefig('../result/01preprocess/03Top Feature Correlation with Label.tif', dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig('../result/01preprocess/03Top Feature Correlation with Label.pdf', dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "data = pd.read_csv(\"../result/01preprocess/01features_eGeMAPS_minmax_drop0.01var.csv\")\n",
    "need_col = sorted_feature_importances[:20].index.tolist()\n",
    "\n",
    "need_col.append(\"label\")\n",
    "data = data[need_col]\n",
    "plot_top_features_heatmap(data, 'label', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix = data.corr()\n",
    "# abs_corr_with_label = abs(corr_matrix['label']).sort_values(ascending=False)\n",
    "# a = abs_corr_with_label[:20].index.tolist()\n",
    "# a.remove(\"label\")\n",
    "# b = sorted_feature_importances[:20].index.tolist()\n",
    "# print(f\"a: {a}\")\n",
    "# print(f\"b: {b}\")\n",
    "\n",
    "# difference_ordered = [x for x in a if x not in b]\n",
    "# print(difference_ordered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 deeplearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tools.common import setup_seed\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import joblib\n",
    "from torchvggish import vggish, vggish_input\n",
    "from joblib import Parallel, delayed, dump\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tools.evaluate import evaluate_model, plot_roc_curve, overall_evaluate_plot, calculate_mean_std_metrics\n",
    "from tools.common import setup_seed\n",
    "from tools.utils import get_eGe_matrix,get_vggish_features\n",
    "from tools.model import EGV_AttNet\n",
    "\n",
    "fold = 5\n",
    "df = pd.read_csv(\"../data/group_control.csv\")\n",
    "shuffled_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "X = shuffled_df['name']\n",
    "y = shuffled_df['class']\n",
    "self_folder = \"../data/group_control/\"\n",
    "\n",
    "save_path = \"../data/enhance/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获得训练集五折的训练特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=fold, shuffle=True)\n",
    "# setup_seed(42)\n",
    "# # 重复实验次数\n",
    "# for _ in range(1):\n",
    "#     # 提前提取所有特征\n",
    "#     X_train_eGe_list = []\n",
    "#     X_test_eGe_list = []\n",
    "#     X_train_VGGish_list = []\n",
    "#     X_test_VGGish_list = []\n",
    "#     y_train_list = []\n",
    "#     y_test_list = []\n",
    "#     train_features_list = []\n",
    "#     test_features_list = []\n",
    "    \n",
    "#     for train_index, test_index in skf.split(X, y):\n",
    "#         print(test_index)\n",
    "#         # 获取 eGeMAPS 特征\n",
    "#         X_train_eGe, y_train, train_features, _ = get_eGe_matrix(train_index[:5], self_folder, X, y, train=True, n_jobs=-1)\n",
    "#         X_test_eGe, y_test, test_features, _ = get_eGe_matrix(test_index[:5], self_folder, X, y, n_jobs=-1)\n",
    "\n",
    "#         # 获取 VGGish 特征\n",
    "#         X_train_VGGish, _, _, _ = get_vggish_features(train_index[:5], self_folder, X, y, train=True, n_jobs=-1)\n",
    "#         X_test_VGGish, _, _, _ = get_vggish_features(test_index[:5], self_folder, X, y, n_jobs=-1)\n",
    "\n",
    "#         #保存特征和标签\n",
    "#         X_train_eGe_list.append(X_train_eGe)\n",
    "#         X_test_eGe_list.append(X_test_eGe)\n",
    "#         X_train_VGGish_list.append(X_train_VGGish)\n",
    "#         X_test_VGGish_list.append(X_test_VGGish)\n",
    "#         y_train_list.append(y_train)\n",
    "#         y_test_list.append(y_test)\n",
    "#         train_features_list.append(train_features)\n",
    "#         test_features_list.append(test_features)\n",
    "\n",
    "        \n",
    "# X_train_eGe_list_path = os.path.join(save_path,\"X_train_eGe_list.joblib\")\n",
    "# X_test_eGe_list_path = os.path.join(save_path,\"X_test_eGe_list.joblib\")\n",
    "# X_train_VGGish_list_path = os.path.join(save_path,\"X_train_VGGish_list.joblib\")\n",
    "# X_test_VGGish_list_path = os.path.join(save_path,\"X_test_VGGish_list.joblib\")\n",
    "# y_train_list_path = os.path.join(save_path,\"y_train_list.joblib\")\n",
    "# y_test_list_path = os.path.join(save_path,\"y_test_list.joblib\")\n",
    "# train_features_list_path = os.path.join(save_path,\"train_features_list.joblib\")\n",
    "# test_features_list_path = os.path.join(save_path,\"test_features_list.joblib\")\n",
    "\n",
    "# # 使用joblib保存增强数据\n",
    "# dump(X_train_eGe_list, X_train_eGe_list_path)\n",
    "# dump(X_test_eGe_list, X_test_eGe_list_path)\n",
    "# dump(X_train_VGGish_list, X_train_VGGish_list_path)\n",
    "# dump(X_test_VGGish_list, X_test_VGGish_list_path)\n",
    "# dump(y_train_list, y_train_list_path)\n",
    "# dump(y_test_list, y_test_list_path)\n",
    "# dump(train_features_list, train_features_list_path) \n",
    "# dump(test_features_list, test_features_list_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "setup_seed(42)\n",
    "def train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test, epochs=50):\n",
    "    \n",
    "    # 将元组中的特征分别提取出来\n",
    "    X_train_eGe, X_train_VGGish = X_train\n",
    "    X_test_eGe, X_test_VGGish = X_test\n",
    "\n",
    "    # 确保输入是 NumPy 数组\n",
    "    X_train_eGe = np.asarray(X_train_eGe)\n",
    "    X_train_VGGish = np.asarray(X_train_VGGish)\n",
    "    X_test_eGe = np.asarray(X_test_eGe)\n",
    "    X_test_VGGish = np.asarray(X_test_VGGish)\n",
    "\n",
    "    # 将特征转换为张量\n",
    "    X_train_eGe = torch.tensor(X_train_eGe, dtype=torch.float32).to(device)\n",
    "    X_train_VGGish = torch.tensor(X_train_VGGish, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)  # CrossEntropyLoss需要长整型标签\n",
    "\n",
    "    X_test_eGe = torch.tensor(X_test_eGe, dtype=torch.float32).to(device)\n",
    "    X_test_VGGish = torch.tensor(X_test_VGGish, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)  # CrossEntropyLoss需要长整型标签\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_eGe, X_train_VGGish)  # 分别传入 eGeMAPS 和 VGGish 特征\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 评估\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test_eGe, X_test_VGGish)  # 分别传入 eGeMAPS 和 VGGish 特征\n",
    "            predictions_prob = val_outputs[:, 1]  # 预测概率          \n",
    "            y_pred = (predictions_prob >= 0.5).int().cpu().numpy() # 预测标签\n",
    "            y_pred_prob = predictions_prob.cpu().numpy()         \n",
    "            evaluta_dic = evaluate_model(y_test.cpu().numpy(), y_pred, y_pred_prob)           \n",
    "    return evaluta_dic   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "save_path = \"../data/enhance/\"\n",
    "\n",
    "X_train_eGe_list_path = os.path.join(save_path,\"X_train_eGe_list.joblib\")\n",
    "X_test_eGe_list_path = os.path.join(save_path,\"X_test_eGe_list.joblib\")\n",
    "X_train_VGGish_list_path = os.path.join(save_path,\"X_train_VGGish_list.joblib\")\n",
    "X_test_VGGish_list_path = os.path.join(save_path,\"X_test_VGGish_list.joblib\")\n",
    "y_train_list_path = os.path.join(save_path,\"y_train_list.joblib\")\n",
    "y_test_list_path = os.path.join(save_path,\"y_test_list.joblib\")\n",
    "train_features_list_path = os.path.join(save_path,\"train_features_list.joblib\")\n",
    "test_features_list_path = os.path.join(save_path,\"test_features_list.joblib\")\n",
    "\n",
    "X_train_eGe_list = joblib.load(X_train_eGe_list_path)\n",
    "X_test_eGe_list = joblib.load(X_test_eGe_list_path)\n",
    "X_train_VGGish_list = joblib.load(X_train_VGGish_list_path) \n",
    "X_test_VGGish_list = joblib.load(X_test_VGGish_list_path)\n",
    "y_train_list = joblib.load(y_train_list_path)\n",
    "y_test_list = joblib.load(y_test_list_path)\n",
    "train_features_list = joblib.load(train_features_list_path)\n",
    "test_features_list = joblib.load(test_features_list_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:18<00:00,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.001, 'weight_decay': 0.001, 'epochs': 90, 'num_conv_layers': 1}\n",
      "Mean f1_score: 0.7477521651649024\n",
      "Mean AUC: 0.8230326985267696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = shuffled_df['class']\n",
    "classes = np.unique(labels)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced', \n",
    "    classes=classes, \n",
    "    y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "evaluate_dics_list = []\n",
    "setup_seed(42)\n",
    "\n",
    "with open(\"../result/01preprocess/eGe_feature.txt\",'r')as f:\n",
    "    eGe_feature = [line.strip() for line in f]\n",
    "\n",
    "feature_weights = pd.read_csv('../result/01preprocess/03sorted_feature_importance.csv',index_col=0)\n",
    "feature_weights = feature_weights.squeeze()\n",
    "feature_weights = feature_weights[eGe_feature]\n",
    "\n",
    "for i in tqdm(range(len(X_train_eGe_list))):\n",
    "        \n",
    "        # 获取预先提取的特征\n",
    "        X_train_eGe = X_train_eGe_list[i]\n",
    "        X_test_eGe = X_test_eGe_list[i]\n",
    "        X_train_VGGish = X_train_VGGish_list[i]\n",
    "        X_test_VGGish = X_test_VGGish_list[i]\n",
    "        y_train = y_train_list[i]\n",
    "        y_test = y_test_list[i]\n",
    "        train_features = train_features_list[i]\n",
    "        test_features = test_features_list[i]\n",
    "\n",
    "        # X-train, X-test\n",
    "        X_train = (X_train_eGe[eGe_feature], X_train_VGGish)\n",
    "        X_test = (X_test_eGe[eGe_feature], X_test_VGGish)\n",
    "\n",
    "        assert feature_weights.index.tolist() == X_train[0].columns.tolist(), \"feature_weights order must be equal to the order of data columns\"\n",
    "\n",
    "        config = {\"lr\": 0.001, \"weight_decay\":0.001, \"epochs\": 90, 'num_conv_layers': 1}\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = EGV_AttNet(input_dim_eGeMAPS=len(feature_weights), input_dim_VGGish=128, num_classes=2, feature_weights=feature_weights, transformed_feature_dim=128, num_conv_layers=config['num_conv_layers']).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        #训练并评估模型\n",
    "        evaluate_dics = train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test, epochs=config['epochs'])\n",
    "        evaluate_dics_list.append(evaluate_dics)\n",
    "print(config)       \n",
    "print(f\"Mean f1_score: {np.mean([evaluate_dics['f1'] for evaluate_dics in evaluate_dics_list])}\")\n",
    "print(f\"Mean AUC: {np.mean([evaluate_dics['roc_auc'] for evaluate_dics in evaluate_dics_list])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tools.common import setup_seed\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import joblib\n",
    "from torchvggish import vggish, vggish_input\n",
    "from joblib import Parallel, delayed, dump\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tools.evaluate import evaluate_model, plot_roc_curve, overall_evaluate_plot, calculate_mean_std_metrics\n",
    "from tools.common import setup_seed,init_logger\n",
    "from tools.utils import get_eGe_matrix,get_vggish_features\n",
    "from tools.model import EGV_AttNet\n",
    "import optuna\n",
    "\n",
    "LOGFILE = f\"../result/02optuna/model_optuna_f1.log\"\n",
    "logger,file_handler = init_logger(LOGFILE)\n",
    "\n",
    "fold = 5\n",
    "df = pd.read_csv(\"../data/group_control.csv\")\n",
    "shuffled_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "X = shuffled_df['name']\n",
    "y = shuffled_df['class']\n",
    "self_folder = \"../data/group_control/\"\n",
    "\n",
    "save_path = \"../data/enhance/\"\n",
    "\n",
    "X_train_eGe_list_path = os.path.join(save_path,\"X_train_eGe_list.joblib\")\n",
    "X_test_eGe_list_path = os.path.join(save_path,\"X_test_eGe_list.joblib\")\n",
    "X_train_VGGish_list_path = os.path.join(save_path,\"X_train_VGGish_list.joblib\")\n",
    "X_test_VGGish_list_path = os.path.join(save_path,\"X_test_VGGish_list.joblib\")\n",
    "y_train_list_path = os.path.join(save_path,\"y_train_list.joblib\")\n",
    "y_test_list_path = os.path.join(save_path,\"y_test_list.joblib\")\n",
    "train_features_list_path = os.path.join(save_path,\"train_features_list.joblib\")\n",
    "test_features_list_path = os.path.join(save_path,\"test_features_list.joblib\")\n",
    "\n",
    "X_train_eGe_list = joblib.load(X_train_eGe_list_path)\n",
    "X_test_eGe_list = joblib.load(X_test_eGe_list_path)\n",
    "X_train_VGGish_list = joblib.load(X_train_VGGish_list_path) \n",
    "X_test_VGGish_list = joblib.load(X_test_VGGish_list_path)\n",
    "y_train_list = joblib.load(y_train_list_path)\n",
    "y_test_list = joblib.load(y_test_list_path)\n",
    "train_features_list = joblib.load(train_features_list_path)\n",
    "test_features_list = joblib.load(test_features_list_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def egv_attnet_optimize():\n",
    "    def objective(trial):\n",
    "        f1_lst = []\n",
    "        for i in tqdm(range(len(X_train_eGe_list))):\n",
    "            X_train_eGe = X_train_eGe_list[i]\n",
    "            X_test_eGe = X_test_eGe_list[i]\n",
    "            X_train_VGGish = X_train_VGGish_list[i]\n",
    "            X_test_VGGish = X_test_VGGish_list[i]\n",
    "            y_train = y_train_list[i]\n",
    "            y_test = y_test_list[i]\n",
    "\n",
    "            X_train = (X_train_eGe[eGe_feature], X_train_VGGish)\n",
    "            X_test = (X_test_eGe[eGe_feature], X_test_VGGish)\n",
    "\n",
    "            assert feature_weights.index.tolist() == X_train[0].columns.tolist(), \"feature_weights order must be equal to the order of data columns\"\n",
    "\n",
    "            model = EGV_AttNet(input_dim_eGeMAPS=len(feature_weights), input_dim_VGGish=128, num_classes=2, feature_weights=feature_weights, transformed_feature_dim=128, num_conv_layers=config['num_conv_layers']).to(device)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "            \n",
    "            #训练并评估模型\n",
    "            evaluate_dics = train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test, epochs=config['epochs'])\n",
    "            evaluate_dics_list.append(evaluate_dics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_handler.close()\n",
    "# optuna-dashboard --host 0.0.0.0 --port 8083 postgresql://postgres:123...@127.0.0.1/depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch_0': [],\n",
       " 'epoch_1': [],\n",
       " 'epoch_2': [],\n",
       " 'epoch_3': [],\n",
       " 'epoch_4': [],\n",
       " 'epoch_5': [],\n",
       " 'epoch_6': [],\n",
       " 'epoch_7': [],\n",
       " 'epoch_8': [],\n",
       " 'epoch_9': []}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=10\n",
    "epoch_lst = {f\"epoch_{i}\":[] for i in range(a)}\n",
    "epoch_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(42)\n",
    "def train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test, epochs=50):\n",
    "    \n",
    "    # 将元组中的特征分别提取出来\n",
    "    X_train_eGe, X_train_VGGish = X_train\n",
    "    X_test_eGe, X_test_VGGish = X_test\n",
    "\n",
    "    # 确保输入是 NumPy 数组\n",
    "    X_train_eGe = np.asarray(X_train_eGe)\n",
    "    X_train_VGGish = np.asarray(X_train_VGGish)\n",
    "    X_test_eGe = np.asarray(X_test_eGe)\n",
    "    X_test_VGGish = np.asarray(X_test_VGGish)\n",
    "\n",
    "    # 将特征转换为张量\n",
    "    X_train_eGe = torch.tensor(X_train_eGe, dtype=torch.float32).to(device)\n",
    "    X_train_VGGish = torch.tensor(X_train_VGGish, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)  # CrossEntropyLoss需要长整型标签\n",
    "\n",
    "    X_test_eGe = torch.tensor(X_test_eGe, dtype=torch.float32).to(device)\n",
    "    X_test_VGGish = torch.tensor(X_test_VGGish, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)  # CrossEntropyLoss需要长整型标签\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_eGe, X_train_VGGish)  # 分别传入 eGeMAPS 和 VGGish 特征\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 评估\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test_eGe, X_test_VGGish)  # 分别传入 eGeMAPS 和 VGGish 特征\n",
    "            predictions_prob = val_outputs[:, 1]  # 预测概率          \n",
    "            y_pred = (predictions_prob >= 0.5).int().cpu().numpy() # 预测标签\n",
    "            y_pred_prob = predictions_prob.cpu().numpy()         \n",
    "            evaluta_dic = evaluate_model(y_test.cpu().numpy(), y_pred, y_pred_prob)           \n",
    "    return evaluta_dic   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = shuffled_df['class']\n",
    "classes = np.unique(labels)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced', \n",
    "    classes=classes, \n",
    "    y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "evaluate_dics_list = []\n",
    "setup_seed(42)\n",
    "\n",
    "with open(\"../result/01preprocess/eGe_feature.txt\",'r')as f:\n",
    "    eGe_feature = [line.strip() for line in f]\n",
    "\n",
    "feature_weights = pd.read_csv('../result/01preprocess/03sorted_feature_importance.csv',index_col=0)\n",
    "feature_weights = feature_weights.squeeze()\n",
    "feature_weights = feature_weights[eGe_feature]\n",
    "\n",
    "for i in tqdm(range(len(X_train_eGe_list))):\n",
    "        \n",
    "        X_train_eGe = X_train_eGe_list[i]\n",
    "        X_test_eGe = X_test_eGe_list[i]\n",
    "        X_train_VGGish = X_train_VGGish_list[i]\n",
    "        X_test_VGGish = X_test_VGGish_list[i]\n",
    "        y_train = y_train_list[i]\n",
    "        y_test = y_test_list[i]\n",
    "        train_features = train_features_list[i]\n",
    "        test_features = test_features_list[i]\n",
    "\n",
    "        X_train = (X_train_eGe[eGe_feature], X_train_VGGish)\n",
    "        X_test = (X_test_eGe[eGe_feature], X_test_VGGish)\n",
    "\n",
    "        assert feature_weights.index.tolist() == X_train[0].columns.tolist(), \"feature_weights order must be equal to the order of data columns\"\n",
    "\n",
    "        config = {\"lr\": 0.001, \"weight_decay\":0.001, \"epochs\": 90, 'num_conv_layers': 1}\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = EGV_AttNet(input_dim_eGeMAPS=len(feature_weights), input_dim_VGGish=128, num_classes=2, feature_weights=feature_weights, transformed_feature_dim=128, num_conv_layers=config['num_conv_layers']).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        #训练并评估模型\n",
    "        evaluate_dics = train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test, epochs=config['epochs'])\n",
    "        evaluate_dics_list.append(evaluate_dics)\n",
    "print(config)       \n",
    "print(f\"Mean f1_score: {np.mean([evaluate_dics['f1'] for evaluate_dics in evaluate_dics_list])}\")\n",
    "print(f\"Mean AUC: {np.mean([evaluate_dics['roc_auc'] for evaluate_dics in evaluate_dics_list])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    evaluate_dics_list = []\n",
    "\n",
    "    for i in range (len(X_train_eGe_list)):\n",
    "            setup_seed(42)\n",
    "            # 获取预先提取的特征\n",
    "            X_train_eGe = X_train_eGe_list[i]\n",
    "            X_test_eGe = X_test_eGe_list[i]\n",
    "            X_train_VGGish = X_train_VGGish_list[i]\n",
    "            X_test_VGGish = X_test_VGGish_list[i]\n",
    "            y_train = y_train_list[i]\n",
    "            y_test = y_test_list[i]\n",
    "            train_features = train_features_list[i]\n",
    "            test_features = test_features_list[i]\n",
    "            \n",
    "            common_features = train_features.intersection(test_features)\n",
    "            # X-train, X-test\n",
    "            X_train = (X_train_eGe[common_features], X_train_VGGish)\n",
    "            X_test = (X_test_eGe[common_features], X_test_VGGish)\n",
    "\n",
    "            # config = {\"lr\": 0.0005, \"weight_decay\":0.004, \"epochs\": 120}\n",
    "\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            model = EGV_AttNet(input_dim_eGeMAPS=len(common_features), input_dim_VGGish=127, num_classes=2, feature_weights=feature_weights, \n",
    "                               num_conv_layers=config['num_conv_layers'],\n",
    "                               conv_out_channels=config['conv_out_channels'], \n",
    "                               transformed_feature_dim=config['transformed_feature_dim'], \n",
    "                               eca_kernel_size=config['eca_kernel_size'], \n",
    "                               resblock_kernel_size=config['resblock_kernel_size']\n",
    "                               ).to(device)\n",
    "            \n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "            \n",
    "            #训练并评估模型\n",
    "            evaluate_dics = train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test, epochs=config['epochs'])\n",
    "            evaluate_dics_list.append(evaluate_dics)\n",
    "    return np.mean([evaluate_dics['f1'] for evaluate_dics in evaluate_dics_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    config = {\"lr\": trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True),\n",
    "              \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True),\n",
    "              \"num_conv_layers\": trial.suggest_int(\"num_conv_layers\", 1, 5, step = 2),\n",
    "              \"conv_out_channels\": trial.suggest_int(\"conv_out_channels\", 32, 256, step = 32),\n",
    "              \"transformed_feature_dim\": trial.suggest_int(\"transformed_feature_dim\", 32, 256, step = 32),\n",
    "              \"eca_kernel_size\": trial.suggest_int(\"eca_kernel_size\", 1, 5, step = 2),\n",
    "              \"resblock_kernel_size\": trial.suggest_int(\"resblock_kernel_size\", 1, 5, step = 2),\n",
    "              \"epochs\": trial.suggest_int(\"epochs\", 80, 120, step = 10),              \n",
    "    }\n",
    "    print(f\"Trial{trial.number} config: {config}\")\n",
    "    f1_score_trial = train(config)\n",
    "    return f1_score_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'code.tools'; 'code' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcode\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m\n\u001b[0;32m      3\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(model)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcode\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EGV_AttNet\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'code.tools'; 'code' is not a package"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import code.tools.model as model\n",
    "importlib.reload(model)\n",
    "from code.tools.model import EGV_AttNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = joblib.load('best_params.pkl')\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(42)\n",
    "st = time.time()\n",
    "study = optuna.create_study(study_name='EGV_AttNet', direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "# 保存最佳超参数\n",
    "joblib.dump(study.best_params, 'best_params.pkl')\n",
    "print('Best params:', study.best_params)\n",
    "print('Best trial:', study.best_trial)\n",
    "print('Best value:', study.best_trial.value)\n",
    "print('Time used:', (time.time() - st)/60,'min')\n",
    "\n",
    "optuna.visualization.plot_param_importances(study).show()\n",
    "optuna.visualization.plot_optimization_history(study).show()\n",
    "optuna.visualization.plot_slice(study).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../data/enhance/\"\n",
    "os.path.join(save_path,\"X_train_eGe_list.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, cohen_kappa_score, roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from scipy.stats import gmean\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from torchvggish import vggish, vggish_input\n",
    "from tools.evaluate import evaluate_model\n",
    "from tools.common import setup_seed\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "save_path = \"../data/enhance/\"\n",
    "\n",
    "X_train_eGe_list_path = os.path.join(save_path,\"X_train_eGe_list.joblib\")\n",
    "X_test_eGe_list_path = os.path.join(save_path,\"X_test_eGe_list.joblib\")\n",
    "X_train_VGGish_list_path = os.path.join(save_path,\"X_train_VGGish_list.joblib\")\n",
    "X_test_VGGish_list_path = os.path.join(save_path,\"X_test_VGGish_list.joblib\")\n",
    "y_train_list_path = os.path.join(save_path,\"y_train_list.joblib\")\n",
    "y_test_list_path = os.path.join(save_path,\"y_test_list.joblib\")\n",
    "train_features_list_path = os.path.join(save_path,\"train_features_list.joblib\")\n",
    "test_features_list_path = os.path.join(save_path,\"test_features_list.joblib\")\n",
    "\n",
    "X_train_eGe_list = joblib.load(X_train_eGe_list_path)\n",
    "X_test_eGe_list = joblib.load(X_test_eGe_list_path)\n",
    "X_train_VGGish_list = joblib.load(X_train_VGGish_list_path) \n",
    "X_test_VGGish_list = joblib.load(X_test_VGGish_list_path)\n",
    "y_train_list = joblib.load(y_train_list_path)\n",
    "y_test_list = joblib.load(y_test_list_path)\n",
    "train_features_list = joblib.load(train_features_list_path)\n",
    "test_features_list = joblib.load(test_features_list_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Mean F1_score of Logistic Regression: 0.6686416086416086\n",
      "Mean AUC of Logistic Regression: 0.8531620553359683\n",
      "Training K-Nearest Neighbors...\n",
      "Mean F1_score of K-Nearest Neighbors: 0.7810223978957028\n",
      "Mean AUC of K-Nearest Neighbors: 0.7968379446640317\n",
      "Training Support Vector Machine...\n",
      "Mean F1_score of Support Vector Machine: 0.7247692217479147\n",
      "Mean AUC of Support Vector Machine: 0.8285842615882141\n",
      "Training Naive Bayes...\n",
      "Mean F1_score of Naive Bayes: 0.11692307692307694\n",
      "Mean AUC of Naive Bayes: 0.5339920948616601\n",
      "Training Decision Tree...\n",
      "Mean F1_score of Decision Tree: 0.6641760676371682\n",
      "Mean AUC of Decision Tree: 0.6849802371541502\n",
      "Training Random Forest...\n",
      "Mean F1_score of Random Forest: 0.6995101164177011\n",
      "Mean AUC of Random Forest: 0.7785932446999642\n",
      "Training Bagging...\n",
      "Mean F1_score of Bagging: 0.7010468499428806\n",
      "Mean AUC of Bagging: 0.7426203736974488\n",
      "Training AdaBoost...\n",
      "Mean F1_score of AdaBoost: 0.7602482933831111\n",
      "Mean AUC of AdaBoost: 0.7925619834710743\n",
      "Training XGBoost...\n",
      "Mean F1_score of XGBoost: 0.7702718934943998\n",
      "Mean AUC of XGBoost: 0.818199784405318\n",
      "Training LightGBM...\n",
      "Mean F1_score of LightGBM: 0.7651986099585362\n",
      "Mean AUC of LightGBM: 0.799595759971254\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(probability=True),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Bagging\": BaggingClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"LightGBM\": LGBMClassifier(verbosity=-1),\n",
    "}\n",
    "\n",
    "with open(\"../result/01preprocess/eGe_feature.txt\",'r')as f:\n",
    "    eGe_feature = [line.strip() for line in f]\n",
    "setup_seed(42)\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    # 建立管道\n",
    "    # pipeline = Pipeline(steps=[('classifier', model)])\n",
    "    # model_constructor = model\n",
    "\n",
    "    evaluate_dics_ml_list = []\n",
    "\n",
    "    for i in range(len(X_train_eGe_list)):\n",
    "        \n",
    "        # 获取预先提取的特征\n",
    "        X_train_eGe = X_train_eGe_list[i]\n",
    "        X_test_eGe = X_test_eGe_list[i]\n",
    "        X_train_VGGish = X_train_VGGish_list[i]\n",
    "        X_test_VGGish = X_test_VGGish_list[i]\n",
    "        y_train = y_train_list[i]\n",
    "        y_test = y_test_list[i]\n",
    "        train_features = train_features_list[i]\n",
    "        test_features = test_features_list[i]\n",
    "        \n",
    "        # 拼接特征\n",
    "        X_train = np.hstack((X_train_eGe[eGe_feature], X_train_VGGish))\n",
    "        X_test = np.hstack((X_test_eGe[eGe_feature], X_test_VGGish))\n",
    "\n",
    "        # 训练模型\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # 预测\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # 评估\n",
    "        evaluate_dics_ml = evaluate_model(y_test, y_pred, y_pred_prob)\n",
    "        evaluate_dics_ml_list.append(evaluate_dics_ml)\n",
    "\n",
    "    print(f\"Mean F1_score of {name}: {np.mean([evaluate_dics_ml['f1'] for evaluate_dics_ml in evaluate_dics_ml_list])}\")\n",
    "    print(f\"Mean AUC of {name}: {np.mean([evaluate_dics_ml['roc_auc'] for evaluate_dics_ml in evaluate_dics_ml_list])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg  width=\"660\" height=\"55\"><rect x=\"0\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#1f77b4;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"55\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#aec7e8;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"110\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#ff7f0e;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"165\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#ffbb78;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"220\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#2ca02c;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"275\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#98df8a;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"330\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#d62728;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"385\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#ff9896;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"440\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#9467bd;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"495\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#c5b0d5;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"550\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#8c564b;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"605\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#c49c94;stroke-width:2;stroke:rgb(255,255,255)\"/></svg>"
      ],
      "text/plain": [
       "[(0.12156862745098039, 0.4666666666666667, 0.7058823529411765),\n",
       " (0.6823529411764706, 0.7803921568627451, 0.9098039215686274),\n",
       " (1.0, 0.4980392156862745, 0.054901960784313725),\n",
       " (1.0, 0.7333333333333333, 0.47058823529411764),\n",
       " (0.17254901960784313, 0.6274509803921569, 0.17254901960784313),\n",
       " (0.596078431372549, 0.8745098039215686, 0.5411764705882353),\n",
       " (0.8392156862745098, 0.15294117647058825, 0.1568627450980392),\n",
       " (1.0, 0.596078431372549, 0.5882352941176471),\n",
       " (0.5803921568627451, 0.403921568627451, 0.7411764705882353),\n",
       " (0.7725490196078432, 0.6901960784313725, 0.8352941176470589),\n",
       " (0.5490196078431373, 0.33725490196078434, 0.29411764705882354),\n",
       " (0.7686274509803922, 0.611764705882353, 0.5803921568627451)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "panel = sns.hls_palette(11,l=.5, s=.6)\n",
    "panel\n",
    "\n",
    "\n",
    "# tab20,Set3\n",
    "panel = sns.color_palette(\"tab20\",12)\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "\"LR\": LogisticRegression(),\n",
    "\"KNN\": KNeighborsClassifier(),\n",
    "\"svm\": SVC(probability=True),\n",
    "\"NB\": GaussianNB(),\n",
    "\"DT\": DecisionTreeClassifier(),\n",
    "\"RF\": RandomForestClassifier(),\n",
    "\"Bagging\": BaggingClassifier(),\n",
    "\"AdaBoost\": AdaBoostClassifier(),\n",
    "\"XGBoost\": XGBClassifier(),\n",
    "\"LightGBM\": LGBMClassifier(verbosity=-1),\n",
    "}\n",
    "b = [1,2,3,4,5,6,7,8,9,10]\n",
    "for a, index in zip(models.items(),b):\n",
    "    c = a[1]\n",
    "    # print(a[1].__name__)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sklearn.linear_model._logistic'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = LogisticRegression()\n",
    "a.__module__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 消融实验 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from without_ECA import EGV_AttNet_without_ECA\n",
    "from without_ResNet import EGV_AttNet_without_ResNet\n",
    "from without_ECA_ResNet import EGV_AttNet_without_ECA_ResNet\n",
    "\n",
    "def train_and_evaluate_model(model, optimizer, criterion, X_train, y_train, X_test, y_test, evaluate_dics_list):    \n",
    "    evaluate_dics = train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test)\n",
    "    evaluate_dics_list.append(evaluate_dics)\n",
    "\n",
    "f1_score_list_without_ECA = []\n",
    "f1_score_list_without_ResNet = []\n",
    "f1_score_list_without_ECA_ResNet = []\n",
    "\n",
    "evaluate_dics_list_without_ECA = []\n",
    "evaluate_dics_list_without_ResNet = []\n",
    "evaluate_dics_list_without_ECA_ResNet = []\n",
    "\n",
    "for i in range(len(X_train_eGe_list)):\n",
    "    setup_seed(42)\n",
    "    # 获取预先提取的特征\n",
    "    X_train_eGe = X_train_eGe_list[i]\n",
    "    X_test_eGe = X_test_eGe_list[i]\n",
    "    X_train_VGGish = X_train_VGGish_list[i]\n",
    "    X_test_VGGish = X_test_VGGish_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "    train_features = train_features_list[i]\n",
    "    test_features = test_features_list[i]\n",
    "   \n",
    "    common_features = train_features.intersection(test_features)\n",
    "\n",
    "    # X-train, X-test\n",
    "    X_train = (X_train_eGe[common_features], X_train_VGGish)\n",
    "    X_test = (X_test_eGe[common_features], X_test_VGGish)\n",
    "    \n",
    "    # 模型1: EGV_AttNet_without_ECA\n",
    "    model_without_ECA = EGV_AttNet_without_ECA(input_dim_eGeMAPS=len(common_features), input_dim_VGGish=127, num_classes=2, feature_weights=feature_weights, num_conv_layers=3).to(device)\n",
    "    optimizer_without_ECA = torch.optim.AdamW(model_without_ECA.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    criterion_without_ECA = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    train_and_evaluate_model(model_without_ECA, optimizer_without_ECA, criterion_without_ECA, X_train, y_train, X_test, y_test, evaluate_dics_list_without_ECA)\n",
    "\n",
    "    # 模型2: EGV_AttNet_without_ResNet\n",
    "    model_without_ResNet = EGV_AttNet_without_ResNet(input_dim_eGeMAPS=len(common_features), input_dim_VGGish=127, num_classes=2, feature_weights=feature_weights, num_conv_layers=3).to(device)\n",
    "    optimizer_without_ResNet = torch.optim.AdamW(model_without_ResNet.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    criterion_without_ResNet = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    train_and_evaluate_model(model_without_ResNet, optimizer_without_ResNet, criterion_without_ResNet, X_train, y_train, X_test, y_test, evaluate_dics_list_without_ResNet)\n",
    "\n",
    "    # 模型3: EGV_AttNet_without_ECA_ResNet\n",
    "    model_without_ECA_ResNet = EGV_AttNet_without_ECA_ResNet(input_dim_eGeMAPS=len(common_features), input_dim_VGGish=127, num_classes=2, feature_weights=feature_weights, num_conv_layers=3).to(device)\n",
    "    optimizer_without_ECA_ResNet = torch.optim.AdamW(model_without_ECA_ResNet.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    criterion_without_ECA_ResNet = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    train_and_evaluate_model(model_without_ECA_ResNet, optimizer_without_ECA_ResNet, criterion_without_ECA_ResNet, X_train, y_train, X_test, y_test, evaluate_dics_list_without_ECA_ResNet)\n",
    "\n",
    "# 打印每个模型的平均f1_score\n",
    "print(f\"Mean f1_score (without ECA): {np.mean(evaluate_dics['f1'] for evaluate_dics in evaluate_dics_list_without_ECA)}\")\n",
    "print(f\"Mean f1_score (without ResNet): {np.mean(evaluate_dics['f1'] for evaluate_dics in evaluate_dics_list_without_ResNet)}\")\n",
    "print(f\"Mean f1_score (without ECA and ResNet): {np.mean(evaluate_dics['f1'] for evaluate_dics in evaluate_dics_list_without_ECA_ResNet)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 tsne 降维分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载eGeMAPS特征数据\n",
    "data = pd.read_csv(r'../result/features_eGeMAPS_去0方差+归一化.csv')  # 文件路径\n",
    "\n",
    "# 最后一列是标签，其余列是特征\n",
    "X = data.iloc[:, :-1].values\n",
    "y =data.iloc[:, -1].values\n",
    "\n",
    "# 标准化特征\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# t-SNE降维到2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# 分别绘制两类数据\n",
    "class_0 = y == 0\n",
    "class_1 = y == 1\n",
    "\n",
    "plt.scatter(X_tsne[class_0, 0], X_tsne[class_0, 1], c='royalblue', label='health', s=50, alpha=0.8)\n",
    "plt.scatter(X_tsne[class_1, 0], X_tsne[class_1, 1], c='tomato', label='unhealth', s=50, alpha=0.8)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('t-SNE Visualization of eGeMAPS Features')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.savefig('../result/figures/tsne/eGeMAPS_t-SNE_Visualization.svg', format='svg', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 3D可视化，可以使用以下代码\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# tsne_3d = TSNE(n_components=3, random_state=42)\n",
    "# X_tsne_3d = tsne_3d.fit_transform(X_scaled)\n",
    "# fig = plt.figure(figsize=(10, 8))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.scatter(X_tsne_3d[class_0, 0], X_tsne_3d[class_0, 1], X_tsne_3d[class_0, 2], c='blue', label='health', s=50, alpha=0.8)\n",
    "# ax.scatter(X_tsne_3d[class_1, 0], X_tsne_3d[class_1, 1], X_tsne_3d[class_1, 2], c='red', label='unhealth', s=50, alpha=0.8)\n",
    "# plt.legend()\n",
    "# plt.title('3D t-SNE Visualization of eGeMAPS Features')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Desktop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
