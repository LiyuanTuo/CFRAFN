{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import joblib\n",
    "from torchvggish import vggish, vggish_input\n",
    "from joblib import Parallel, delayed, dump\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tools.evaluate import evaluate_model, plot_roc_curve, overall_evaluate_plot, calculate_mean_std_metrics\n",
    "from tools.common import setup_seed,init_logger\n",
    "from tools.utils import get_eGe_matrix,get_vggish_features,get_best_para_from_optuna\n",
    "# from tools.model import EGV_AttNet\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import swanlab\n",
    "from swanlab.plugin.notification import WXWorkCallback\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/group_control.csv\")\n",
    "shuffled_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "LOGFILE = f\"../result/02optuna/depression_model_performance622.log\"\n",
    "logger,file_handler = init_logger(LOGFILE)\n",
    "\n",
    "X = shuffled_df['name']\n",
    "y = shuffled_df['class']\n",
    "self_folder = \"../data/group_control/\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "setup_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(save_path):\n",
    "    X_train_eGe_path = os.path.join(save_path,\"X_train_eGe.joblib\")\n",
    "    X_test_eGe_path = os.path.join(save_path,\"X_test_eGe.joblib\")\n",
    "    X_val_eGe_path = os.path.join(save_path,\"X_val_eGe.joblib\")\n",
    "    X_train_VGGish_path = os.path.join(save_path,\"X_train_VGGish.joblib\")\n",
    "    X_test_VGGish_path = os.path.join(save_path,\"X_test_VGGish.joblib\")\n",
    "    X_val_VGGish_path = os.path.join(save_path,\"X_val_VGGish.joblib\")\n",
    "    y_train_path = os.path.join(save_path,\"y_train.joblib\")\n",
    "    y_test_path = os.path.join(save_path,\"y_test.joblib\")\n",
    "    y_val_path = os.path.join(save_path,\"y_val.joblib\")\n",
    "\n",
    "    X_train_eGe = joblib.load(X_train_eGe_path)\n",
    "    X_test_eGe = joblib.load(X_test_eGe_path)\n",
    "    X_val_eGe = joblib.load(X_val_eGe_path)\n",
    "    X_train_VGGish = joblib.load(X_train_VGGish_path) \n",
    "    X_test_VGGish = joblib.load(X_test_VGGish_path)\n",
    "    X_val_VGGish = joblib.load(X_val_VGGish_path)\n",
    "    y_train = joblib.load(y_train_path)\n",
    "    y_test = joblib.load(y_test_path)\n",
    "    y_val = joblib.load(y_val_path)\n",
    "    return X_train_eGe,X_test_eGe,X_val_eGe,X_train_VGGish,X_test_VGGish,X_val_VGGish,y_train,y_test,y_val\n",
    "\n",
    "\n",
    "with open(\"../result/01preprocess/eGe_feature_cumul0.99.txt\",'r')as f:\n",
    "    eGe_feature = [line.strip() for line in f]\n",
    "\n",
    "feature_weights = pd.read_csv('../result/01preprocess/03sorted_feature_importance.csv',index_col=0)\n",
    "feature_weights = feature_weights.squeeze()\n",
    "feature_weights = feature_weights[eGe_feature]\n",
    "\n",
    "labels = shuffled_df['class']\n",
    "classes = np.unique(labels)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "print(labels.sum())\n",
    "# class_weights = torch.tensor(class_weights, dtype=torch.float, device=device)\n",
    "class_weights = torch.tensor(np.array([100,1]), dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test, \n",
    "                   sample_weight=0.5, epochs=50, patience=10, save_flag=False):\n",
    "    setup_seed(42)\n",
    "    best_f1 = 0.0  # 用于记录最佳 F1 分数\n",
    "    no_improvement_count = 0  # 记录未提升的轮数\n",
    "\n",
    "    # 将元组中的特征分别提取出来\n",
    "    X_train_eGe, X_train_VGGish = X_train\n",
    "    X_test_eGe, X_test_VGGish = X_test\n",
    "\n",
    "    # 确保输入是 NumPy 数组\n",
    "    X_train_eGe = np.asarray(X_train_eGe)\n",
    "    X_train_VGGish = np.asarray(X_train_VGGish)\n",
    "    X_test_eGe = np.asarray(X_test_eGe)\n",
    "    X_test_VGGish = np.asarray(X_test_VGGish)\n",
    "\n",
    "    # 将特征转换为张量\n",
    "    X_train_eGe = torch.tensor(X_train_eGe, dtype=torch.float32).to(device)\n",
    "    X_train_VGGish = torch.tensor(X_train_VGGish, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "    X_test_eGe = torch.tensor(X_test_eGe, dtype=torch.float32).to(device)\n",
    "    X_test_VGGish = torch.tensor(X_test_VGGish, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    groups = [list(range(i, min(i+6, len(X_train_eGe)))) for i in range(0, len(X_train_eGe), 6)] \n",
    "    \n",
    "    swanlab.init(\n",
    "        project=\"depression\",\n",
    "        workspace=\"2283118171\",\n",
    "        experiment_name=\"ATT 随机加权 optuna 6000\",\n",
    "        description=\"不进行标准化且去除音量增强。改为二元交叉熵损失BCEloss  设置小学习率和大epoch\",\n",
    "        # mode='disabled', # 调试模式\n",
    "        # callbacks=[wxwork_callback]\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        # # 初始化选中索引列表（保留所有原始样本）\n",
    "        # selected_indices = list(range(0, num_samples, 6))  # 所有组的第1个样本（原始数据）\n",
    "\n",
    "        # # 为每组随机选1个增强样本\n",
    "        # for group_start in range(0, num_samples, 6):\n",
    "        #     # 随机从该组的增强样本（1-5）中选1个\n",
    "        #     augment_index = random.randint(group_start + 1, group_start + 5)\n",
    "        #     selected_indices.append(augment_index)\n",
    "        \n",
    "        selected_indices = [random.choices(group, weights=[sample_weight] + [(1-sample_weight)/(len(group)-1)]*(len(group)-1), k=1)[0] for group in groups]\n",
    "\n",
    "        # 获取选中的数据\n",
    "        X_train_eGe_select = X_train_eGe[selected_indices]\n",
    "        X_train_VGGish_select = X_train_VGGish[selected_indices]\n",
    "        y_train_select = y_train[selected_indices]\n",
    "\n",
    "        \n",
    "        # 随机取\n",
    "\n",
    "        # outputs = model(X_train_eGe, X_train_VGGish).squeeze()  # 分别传入 eGeMAPS 和 VGGish 特征\n",
    "        outputs = model(X_train_eGe_select, X_train_VGGish_select).squeeze() \n",
    "        # loss = criterion(outputs, y_train)\n",
    "        loss = criterion(outputs, y_train_select)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        swanlab.log({\"train/loss\": loss}, step=epoch)\n",
    "\n",
    "        # 评估\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test_eGe, X_test_VGGish).squeeze()  # 分别传入 eGeMAPS 和 VGGish 特征\n",
    "            loss_test = criterion(val_outputs, y_test) \n",
    "            predictions_prob = val_outputs # 预测概率       \n",
    "            y_pred = (predictions_prob >= 0.5).int().cpu().numpy() # 预测标签\n",
    "            y_pred_prob = predictions_prob.cpu().numpy()         \n",
    "            evaluta_dic = evaluate_model(y_test.cpu().numpy(), y_pred, y_pred_prob)\n",
    "            swanlab.log({\"test/loss\": loss_test})\n",
    "            swanlab.log({\"test/f1\": evaluta_dic['f1']})\n",
    "            swanlab.log({\"test/auc\": evaluta_dic['roc_auc']})\n",
    "            swanlab.log({\"test/acc\": evaluta_dic['acc']})\n",
    "\n",
    "\n",
    "        # print(f\"{epoch} ==> loss: {loss:.4f}    f1; {evaluta_dic['f1']:.4f}\")\n",
    "        \n",
    "        # 早停检测\n",
    "        if evaluta_dic['f1'] > best_f1:\n",
    "            best_f1 = evaluta_dic['f1']\n",
    "            best_auc = evaluta_dic['roc_auc']\n",
    "            no_improvement_count = 0  # 重置计数器\n",
    "            swanlab.log({\"test/early_stop_epoch\": epoch})\n",
    "            swanlab.log({\"test/early_stop_f1\": best_f1})\n",
    "            swanlab.log({\"test/early_stop_auc\": best_auc})\n",
    "            if save_flag:\n",
    "                torch.save(model.state_dict(), \"../result/04modelPerformance/best_EgvAtt_model.pth\")  # 保存当前最佳模型\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        # 如果没有提升的轮数超过阈值，停止训练\n",
    "        if no_improvement_count >= patience:\n",
    "            # print(f\"Early stopping triggered. Best F1: {best_f1:.4f}\")\n",
    "            break\n",
    "    # torch.save(model.state_dict(), \"../result/04modelPerformance/best_EgvAtt_model.pth\")\n",
    "    swanlab.finish()\n",
    "    return best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_model(model_param=None):\n",
    "    setup_seed(42)\n",
    "    # lr = model_param['lr']\n",
    "    # transformed_feature_dim = model_param['transformed_feature_dim']\n",
    "    # weight_decay = model_param['weight_decay']\n",
    "    # conv_out_channels = model_param['conv_out_channels']\n",
    "    # epoch = model_param['epoch']\n",
    "    # patience = model_param['patience']\n",
    "    \n",
    "    # lr = model_param['lr']\n",
    "    # sample_weight = model_param['sample_weight']\n",
    "    # conv_out_channels = model_param['conv_out_channels']\n",
    "    # transformed_feature_dim = model_param['transformed_feature_dim']\n",
    "    # cls_dim = model_param['cls_dim']\n",
    "    lr = 0.00044408314231896245\n",
    "    conv_out_channels = 104\n",
    "    transformed_feature_dim = 250\n",
    "    cls_dim =207\n",
    "    sample_weight = 0.6\n",
    "\n",
    "    weight_decay = 2e-4\n",
    "    expansion = 2\n",
    "    dropout = 0.1\n",
    "    resblock_kernel_size = 5\n",
    "    epoch = 2000\n",
    "    patience = 200\n",
    "    \n",
    "\n",
    "    # swanlab.config = {\n",
    "    #     \"lr\": lr,\n",
    "    #     \"transformed_feature_dim\": transformed_feature_dim,\n",
    "    #     \"weight_decay\": weight_decay,\n",
    "    #     \"epoch\": epoch,\n",
    "    #     \"patience\": patience,\n",
    "    #     \"conv_out_channels\": conv_out_channels,\n",
    "    #     \"eca_kernel_size\": eca_kernel_size,\n",
    "    #     \"resblock_kernel_size\": resblock_kernel_size,\n",
    "    # }\n",
    "\n",
    "    # data_path = \"../data/modelBasicPerformance/dropout\"  # 没有做归一并且去除了音量增强\n",
    "    data_path = \"../data/modelBasicPerformance/\"  # 没有做归一并且去除了音量增强\n",
    "    X_train_eGe,X_test_eGe,X_val_eGe,X_train_VGGish,X_test_VGGish,X_val_VGGish,y_train,y_test,y_val = read_data(data_path)\n",
    "\n",
    "    X_train = (X_train_eGe[eGe_feature], X_train_VGGish)\n",
    "    X_test = (X_test_eGe[eGe_feature], X_test_VGGish)\n",
    "\n",
    "    assert feature_weights.index.tolist() == X_train[0].columns.tolist(), \"feature_weights order must be equal to the order of data columns\"\n",
    "    \n",
    "    model = EGV_AttNet(input_dim_eGeMAPS=len(feature_weights), input_dim_VGGish=128, \n",
    "                       feature_weights=feature_weights, expansion=expansion, dropout=dropout,\n",
    "                       conv_out_channels=conv_out_channels,\n",
    "                       transformed_feature_dim=transformed_feature_dim, \n",
    "                       resblock_kernel_size=resblock_kernel_size, cls_dim=cls_dim).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # if criterion == \"bce\":\n",
    "    #     criterion = nn.BCELoss()\n",
    "    # elif criterion == 'focal':\n",
    "    #     focal_alpha = model_param['focal_alpha']\n",
    "    #     criterion = FocalLoss(alpha=focal_alpha)\n",
    "    # else:\n",
    "    #     NotImplementedError\n",
    "        \n",
    "    #训练并评估模型\n",
    "    f1 = train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test, \n",
    "                        sample_weight=sample_weight, epochs=epoch, patience=patience, \n",
    "                        save_flag=True)\n",
    "    \n",
    "    # print(f\"best f1: {f1}\")\n",
    "    def _plot_confusion_matrix(y_true, y_pred, classes, dataset_name, cmap=plt.cm.Blues):\n",
    "        plt.rcParams['font.family'] = 'Arial'\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, \n",
    "                    xticklabels=classes, yticklabels=classes)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "\n",
    "        plt.savefig(f'../result/04modelPerformance/01confusion_matrix_{dataset_name}.tif', dpi=300, bbox_inches=\"tight\")\n",
    "        plt.savefig(f'../result/04modelPerformance/01confusion_matrix_{dataset_name}.pdf', dpi=300, bbox_inches=\"tight\")\n",
    "        \n",
    "        # plt.show()\n",
    "\n",
    "    def _get_evaluate_dic(eGe, vgg, y_true, dataset_name, save_path):\n",
    "        outputs = model(eGe, vgg)\n",
    "        predictions_prob = outputs\n",
    "        y_pred = (predictions_prob >= 0.5).int().cpu().numpy()\n",
    "        y_pred_prob = predictions_prob.cpu().numpy()         \n",
    "        evaluate_dic = evaluate_model(y_true.cpu().numpy(), y_pred, y_pred_prob)\n",
    "        print(f\"{dataset_name} dataset:\\tF1: {evaluate_dic['f1']:.4f}\\tAUC: {evaluate_dic['roc_auc']:.4f}\\tACC: {evaluate_dic['acc']:.4f}\")\n",
    "        _plot_confusion_matrix(y_true.cpu().numpy(), y_pred, 2, dataset_name)\n",
    "\n",
    "        del evaluate_dic['fpr']\n",
    "        del evaluate_dic['tpr']\n",
    "        df = pd.DataFrame(evaluate_dic, index=[0])\n",
    "        \n",
    "        cols = ['f1','roc_auc','aupr','gmean','kappa','mcc','acc','npv','ppv','sensitivity','specificity']\n",
    "        df = df[cols]\n",
    "        df.to_excel(os.path.join(save_path, f\"{dataset_name}_evaluate.xlsx\"), index=False)\n",
    "\n",
    "\n",
    "\n",
    "    model.load_state_dict(torch.load(\"../result/04modelPerformance/best_EgvAtt_model.pth\"))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        X_test_eGe = np.asarray(X_test_eGe[eGe_feature])\n",
    "        X_val_eGe = np.asarray(X_val_eGe[eGe_feature])\n",
    "\n",
    "        X_test_VGGish = np.asarray(X_test_VGGish)\n",
    "        X_val_VGGish = np.asarray(X_val_VGGish)\n",
    "        # print(f\"y_test: {y_test}\")\n",
    "        # print(f\"y_val: {y_val}\")\n",
    "        # 将特征转换为张量\n",
    "        X_test_eGe = torch.tensor(X_test_eGe, dtype=torch.float32).to(device)\n",
    "        X_test_VGGish = torch.tensor(X_test_VGGish, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "        X_val_eGe = torch.tensor(X_val_eGe, dtype=torch.float32).to(device)\n",
    "        X_val_VGGish = torch.tensor(X_val_VGGish, dtype=torch.float32).to(device)\n",
    "        y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "        save_path = '../result/04modelPerformance/'\n",
    "        _get_evaluate_dic(X_test_eGe, X_test_VGGish, y_test, \"val\", save_path)\n",
    "        _get_evaluate_dic(X_val_eGe, X_val_VGGish, y_val, \"test\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 超参数优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    setup_seed(42)\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-3, log=True)  # 2e-4\n",
    "    conv_out_channels = trial.suggest_int('conv_out_channels', 32, 256)\n",
    "    transformed_feature_dim = trial.suggest_int('transformed_feature_dim', 64, 512)\n",
    "    cls_dim = trial.suggest_int('cls_dim', 64, 256)\n",
    "    sample_weight = trial.suggest_float('sample_weight', 0.01, 0.99)\n",
    "    \n",
    "    weight_decay = 2e-4\n",
    "    expansion = 2\n",
    "    dropout = 0.1\n",
    "    resblock_kernel_size = 5\n",
    "\n",
    "    epoch = 2000\n",
    "    patience = 200\n",
    "\n",
    "    # data_path = \"../data/modelBasicPerformance/dropvoice/\"\n",
    "    data_path = \"../data/modelBasicPerformance/\"\n",
    "    X_train_eGe,X_test_eGe,X_val_eGe,X_train_VGGish,X_test_VGGish,X_val_VGGish,y_train,y_test,y_val = read_data(data_path)\n",
    "\n",
    "    X_train = (X_train_eGe[eGe_feature], X_train_VGGish)\n",
    "    X_test = (X_test_eGe[eGe_feature], X_test_VGGish)\n",
    "\n",
    "    assert feature_weights.index.tolist() == X_train[0].columns.tolist(), \"feature_weights order must be equal to the order of data columns\"\n",
    "    \n",
    "    model = EGV_AttNet(input_dim_eGeMAPS=len(feature_weights), input_dim_VGGish=128, \n",
    "                       feature_weights=feature_weights, expansion=expansion, dropout=dropout,\n",
    "                       conv_out_channels=conv_out_channels,\n",
    "                       transformed_feature_dim=transformed_feature_dim, \n",
    "                       resblock_kernel_size=resblock_kernel_size, cls_dim=cls_dim).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    f1 = train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test, \n",
    "                        sample_weight=sample_weight, epochs=epoch, patience=patience)\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 主程序部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mode=\"train\"):\n",
    "\n",
    "    setup_seed(42)\n",
    "    storage_name = \"postgresql://Sy:qwe123@127.0.0.1:5432/depression\"\n",
    "    # EgvAttNet\n",
    "    study_name = \"EgvAttNet\"\n",
    "\n",
    "    def optimize(n_trials):\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    if mode == \"train\":\n",
    "        try:\n",
    "            optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "            optuna.delete_study(study_name=study_name, storage=storage_name)\n",
    "            print(f\"Deleted existing study: {study_name}\")\n",
    "        except KeyError:\n",
    "            print(f\"Study {study_name} does not exist, creating new one.\")\n",
    "        study = optuna.create_study(study_name=study_name, storage=storage_name, sampler=optuna.samplers.TPESampler(seed=42), direction='maximize')\n",
    "        # study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "        # study.optimize(objective, n_trials=10, n_jobs=10, show_progress_bar=True)\n",
    "        # Parallel(n_jobs=10)([delayed(optimize)(30) for _ in range(1)]) # 多线程并行运行\n",
    "        \n",
    "        study.optimize(objective, n_trials=20) # 单线程运行\n",
    "        logger.info(f\"Best parameters: {study.best_params}\")\n",
    "        logger.info(f\"Best score: {study.best_value}\")\n",
    "        file_handler.close()\n",
    "    elif mode == \"test\":\n",
    "        best_params = get_best_para_from_optuna(study_name=study_name, storage_name=storage_name)\n",
    "        evaluate_test_model(best_params)\n",
    "        # evaluate_test_model()\n",
    "    else:\n",
    "        NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    setup_seed(42)\n",
    "    # import argparse\n",
    "    # parser = argparse.ArgumentParser(description=\"Run the model with optional tuning.\")\n",
    "    \n",
    "    # # 2. 添加 --tuning 参数（类型为 int，默认 0）\n",
    "    # parser.add_argument(\n",
    "    #     \"--mode\",\n",
    "    #     type=str,\n",
    "    #     default=\"train\",\n",
    "    #     help=\"Enable tuning mode (train or test).\"\n",
    "    # )\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    # run = swanlab.init(\n",
    "    #     project=\"depression\",\n",
    "    #     experiment_name=\"ATT 随机加权 optuna 6000\",\n",
    "    #     description=\"不进行标准化且去除音量增强。改为二元交叉熵损失BCEloss  设置小学习率和大epoch\",\n",
    "    #     mode='disabled', # 调试模式\n",
    "    #     # callbacks=[wxwork_callback]\n",
    "    # )\n",
    "\n",
    "    main(mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 简单的特征融合：加俩特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from tools.config import model_paths\n",
    "\n",
    "def load_model(model_name):\n",
    "    module_path = model_paths[model_name]\n",
    "    module = importlib.import_module(module_path)\n",
    "    return getattr(module, 'EGV_AttNet')\n",
    "\n",
    "# 加载模型\n",
    "EGV_AttNet_Class = load_model('EGV_AttNet')\n",
    "# EGV_AttNet_Class = load_model('simplecombine')\n",
    "# EGV_AttNet_Class = load_model('eGe_Net')\n",
    "# EGV_AttNet_Class = load_model('vgg_Net')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test, \n",
    "                   sample_weight=0.5, epochs=50, patience=10, save_flag=False):\n",
    "    setup_seed(42)\n",
    "    best_f1 = 0.0  # 用于记录最佳 F1 分数\n",
    "    no_improvement_count = 0  # 记录未提升的轮数\n",
    "\n",
    "    # 将元组中的特征分别提取出来\n",
    "    X_train_eGe, X_train_VGGish = X_train\n",
    "    X_test_eGe, X_test_VGGish = X_test\n",
    "\n",
    "    # 确保输入是 NumPy 数组\n",
    "    X_train_eGe = np.asarray(X_train_eGe)\n",
    "    X_train_VGGish = np.asarray(X_train_VGGish)\n",
    "    X_test_eGe = np.asarray(X_test_eGe)\n",
    "    X_test_VGGish = np.asarray(X_test_VGGish)\n",
    "\n",
    "    # 将特征转换为张量\n",
    "    X_train_eGe = torch.tensor(X_train_eGe, dtype=torch.float32).to(device)\n",
    "    X_train_VGGish = torch.tensor(X_train_VGGish, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "    X_test_eGe = torch.tensor(X_test_eGe, dtype=torch.float32).to(device)\n",
    "    X_test_VGGish = torch.tensor(X_test_VGGish, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    groups = [list(range(i, min(i+6, len(X_train_eGe)))) for i in range(0, len(X_train_eGe), 6)] \n",
    "    \n",
    "    swanlab.init(\n",
    "        project=\"depression\",\n",
    "        workspace=\"2283118171\",\n",
    "        experiment_name=\"ATT 随机加权 optuna 6000\",\n",
    "        description=\"不进行标准化且去除音量增强。改为二元交叉熵损失BCEloss  设置小学习率和大epoch\",\n",
    "        # mode='disabled', # 调试模式\n",
    "        # callbacks=[wxwork_callback]\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        # # 初始化选中索引列表（保留所有原始样本）\n",
    "        # selected_indices = list(range(0, num_samples, 6))  # 所有组的第1个样本（原始数据）\n",
    "\n",
    "        # # 为每组随机选1个增强样本\n",
    "        # for group_start in range(0, num_samples, 6):\n",
    "        #     # 随机从该组的增强样本（1-5）中选1个\n",
    "        #     augment_index = random.randint(group_start + 1, group_start + 5)\n",
    "        #     selected_indices.append(augment_index)\n",
    "        \n",
    "        selected_indices = [random.choices(group, weights=[sample_weight] + [(1-sample_weight)/(len(group)-1)]*(len(group)-1), k=1)[0] for group in groups]\n",
    "\n",
    "        # 获取选中的数据\n",
    "        X_train_eGe_select = X_train_eGe[selected_indices]\n",
    "        X_train_VGGish_select = X_train_VGGish[selected_indices]\n",
    "        y_train_select = y_train[selected_indices]\n",
    "\n",
    "        \n",
    "        # 随机取\n",
    "\n",
    "        # outputs = model(X_train_eGe, X_train_VGGish).squeeze()  # 分别传入 eGeMAPS 和 VGGish 特征\n",
    "        outputs = model(X_train_eGe_select, X_train_VGGish_select).squeeze() \n",
    "        # loss = criterion(outputs, y_train)\n",
    "        loss = criterion(outputs, y_train_select)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        swanlab.log({\"train/loss\": loss}, step=epoch)\n",
    "\n",
    "        # 评估\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test_eGe, X_test_VGGish).squeeze()  # 分别传入 eGeMAPS 和 VGGish 特征\n",
    "            loss_test = criterion(val_outputs, y_test) \n",
    "            predictions_prob = val_outputs # 预测概率       \n",
    "            y_pred = (predictions_prob >= 0.5).int().cpu().numpy() # 预测标签\n",
    "            y_pred_prob = predictions_prob.cpu().numpy()         \n",
    "            evaluta_dic = evaluate_model(y_test.cpu().numpy(), y_pred, y_pred_prob)\n",
    "            swanlab.log({\"test/loss\": loss_test})\n",
    "            swanlab.log({\"test/f1\": evaluta_dic['f1']})\n",
    "            swanlab.log({\"test/auc\": evaluta_dic['roc_auc']})\n",
    "            swanlab.log({\"test/acc\": evaluta_dic['acc']})\n",
    "\n",
    "\n",
    "        # print(f\"{epoch} ==> loss: {loss:.4f}    f1; {evaluta_dic['f1']:.4f}\")\n",
    "        \n",
    "        # 早停检测\n",
    "        if evaluta_dic['f1'] > best_f1:\n",
    "            best_f1 = evaluta_dic['f1']\n",
    "            best_auc = evaluta_dic['roc_auc']\n",
    "            no_improvement_count = 0  # 重置计数器\n",
    "            swanlab.log({\"test/early_stop_epoch\": epoch})\n",
    "            swanlab.log({\"test/early_stop_f1\": best_f1})\n",
    "            swanlab.log({\"test/early_stop_auc\": best_auc})\n",
    "            if save_flag:\n",
    "                torch.save(model.state_dict(), \"../result/04modelPerformance/best_EgvAtt_model.pth\")  # 保存当前最佳模型\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        # 如果没有提升的轮数超过阈值，停止训练\n",
    "        if no_improvement_count >= patience:\n",
    "            # print(f\"Early stopping triggered. Best F1: {best_f1:.4f}\")\n",
    "            break\n",
    "    # torch.save(model.state_dict(), \"../result/04modelPerformance/best_EgvAtt_model.pth\")\n",
    "    swanlab.finish()\n",
    "    return best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_model(model_param=None):\n",
    "    setup_seed(42)\n",
    "    # lr = model_param['lr']\n",
    "    # transformed_feature_dim = model_param['transformed_feature_dim']\n",
    "    # weight_decay = model_param['weight_decay']\n",
    "    # conv_out_channels = model_param['conv_out_channels']\n",
    "    # epoch = model_param['epoch']\n",
    "    # patience = model_param['patience']\n",
    "    \n",
    "    # lr = model_param['lr']\n",
    "    # sample_weight = model_param['sample_weight']\n",
    "    # conv_out_channels = model_param['conv_out_channels']\n",
    "    # transformed_feature_dim = model_param['transformed_feature_dim']\n",
    "    # cls_dim = model_param['cls_dim']\n",
    "    lr = 0.00044408314231896245\n",
    "    conv_out_channels = 104\n",
    "    transformed_feature_dim = 250\n",
    "    cls_dim =207\n",
    "    sample_weight = 0.6\n",
    "\n",
    "    weight_decay = 2e-4\n",
    "    expansion = 2\n",
    "    dropout = 0.1\n",
    "    resblock_kernel_size = 5\n",
    "    epoch = 2000\n",
    "    patience = 200\n",
    "    \n",
    "\n",
    "    # swanlab.config = {\n",
    "    #     \"lr\": lr,\n",
    "    #     \"transformed_feature_dim\": transformed_feature_dim,\n",
    "    #     \"weight_decay\": weight_decay,\n",
    "    #     \"epoch\": epoch,\n",
    "    #     \"patience\": patience,\n",
    "    #     \"conv_out_channels\": conv_out_channels,\n",
    "    #     \"eca_kernel_size\": eca_kernel_size,\n",
    "    #     \"resblock_kernel_size\": resblock_kernel_size,\n",
    "    # }\n",
    "\n",
    "    # data_path = \"../data/modelBasicPerformance/dropout\"  # 没有做归一并且去除了音量增强\n",
    "    data_path = \"../data/modelBasicPerformance/\"  # 没有做归一并且去除了音量增强\n",
    "    X_train_eGe,X_test_eGe,X_val_eGe,X_train_VGGish,X_test_VGGish,X_val_VGGish,y_train,y_test,y_val = read_data(data_path)\n",
    "\n",
    "    X_train = (X_train_eGe[eGe_feature], X_train_VGGish)\n",
    "    X_test = (X_test_eGe[eGe_feature], X_test_VGGish)\n",
    "\n",
    "    assert feature_weights.index.tolist() == X_train[0].columns.tolist(), \"feature_weights order must be equal to the order of data columns\"\n",
    "    \n",
    "    model = EGV_AttNet_Class(input_dim_eGeMAPS=len(feature_weights), input_dim_VGGish=128, \n",
    "                       feature_weights=feature_weights, expansion=expansion, dropout=dropout,\n",
    "                       conv_out_channels=conv_out_channels,\n",
    "                       transformed_feature_dim=transformed_feature_dim, \n",
    "                       resblock_kernel_size=resblock_kernel_size, cls_dim=cls_dim).to(device)\n",
    "    \n",
    "    print(f\"Model: {EGV_AttNet_Class.__name__}\")\n",
    "\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # if criterion == \"bce\":\n",
    "    #     criterion = nn.BCELoss()\n",
    "    # elif criterion == 'focal':\n",
    "    #     focal_alpha = model_param['focal_alpha']\n",
    "    #     criterion = FocalLoss(alpha=focal_alpha)\n",
    "    # else:\n",
    "    #     NotImplementedError\n",
    "        \n",
    "    #训练并评估模型\n",
    "    f1 = train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test, \n",
    "                        sample_weight=sample_weight, epochs=epoch, patience=patience, \n",
    "                        save_flag=True)\n",
    "     \n",
    "    # print(f\"best f1: {f1}\")\n",
    "    def _plot_confusion_matrix(y_true, y_pred, classes, dataset_name, cmap=plt.cm.Blues):\n",
    "        plt.rcParams['font.family'] = 'Arial'\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, \n",
    "                    xticklabels=classes, yticklabels=classes)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "\n",
    "        plt.savefig(f'../result/04modelPerformance/01confusion_matrix_{dataset_name}.tif', dpi=300, bbox_inches=\"tight\")\n",
    "        plt.savefig(f'../result/04modelPerformance/01confusion_matrix_{dataset_name}.pdf', dpi=300, bbox_inches=\"tight\")\n",
    "        \n",
    "        # plt.show()\n",
    "\n",
    "    def _get_evaluate_dic(eGe, vgg, y_true, dataset_name, save_path):\n",
    "        outputs = model(eGe, vgg)\n",
    "        predictions_prob = outputs\n",
    "        y_pred = (predictions_prob >= 0.5).int().cpu().numpy()\n",
    "        y_pred_prob = predictions_prob.cpu().numpy()         \n",
    "        evaluate_dic = evaluate_model(y_true.cpu().numpy(), y_pred, y_pred_prob)\n",
    "        print(f\"{dataset_name} dataset:\\tF1: {evaluate_dic['f1']:.4f}\\tAUC: {evaluate_dic['roc_auc']:.4f}\\tACC: {evaluate_dic['acc']:.4f}\")\n",
    "        _plot_confusion_matrix(y_true.cpu().numpy(), y_pred, 2, dataset_name)\n",
    "\n",
    "        del evaluate_dic['fpr']\n",
    "        del evaluate_dic['tpr']\n",
    "        df = pd.DataFrame(evaluate_dic, index=[0])\n",
    "        \n",
    "        cols = ['f1','roc_auc','aupr','gmean','kappa','mcc','acc','npv','ppv','sensitivity','specificity']\n",
    "        df = df[cols]\n",
    "        df.to_excel(os.path.join(save_path, f\"{dataset_name}_evaluate.xlsx\"), index=False)\n",
    "\n",
    "\n",
    "\n",
    "    model.load_state_dict(torch.load(\"../result/04modelPerformance/best_EgvAtt_model.pth\"))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        X_test_eGe = np.asarray(X_test_eGe[eGe_feature])\n",
    "        X_val_eGe = np.asarray(X_val_eGe[eGe_feature])\n",
    "\n",
    "        X_test_VGGish = np.asarray(X_test_VGGish)\n",
    "        X_val_VGGish = np.asarray(X_val_VGGish)\n",
    "        # print(f\"y_test: {y_test}\")\n",
    "        # print(f\"y_val: {y_val}\")\n",
    "        # 将特征转换为张量\n",
    "        X_test_eGe = torch.tensor(X_test_eGe, dtype=torch.float32).to(device)\n",
    "        X_test_VGGish = torch.tensor(X_test_VGGish, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "        X_val_eGe = torch.tensor(X_val_eGe, dtype=torch.float32).to(device)\n",
    "        X_val_VGGish = torch.tensor(X_val_VGGish, dtype=torch.float32).to(device)\n",
    "        y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "        save_path = '../result/04modelPerformance/'\n",
    "        _get_evaluate_dic(X_test_eGe, X_test_VGGish, y_test, \"val\", save_path)\n",
    "        _get_evaluate_dic(X_val_eGe, X_val_VGGish, y_val, \"test\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mode=\"train\"):\n",
    "\n",
    "    setup_seed(42)\n",
    "    storage_name = \"postgresql://Sy:qwe123@127.0.0.1:5432/depression\"\n",
    "    # EgvAttNet\n",
    "    study_name = \"EgvAttNet\"\n",
    "\n",
    "    def optimize(n_trials):\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    if mode == \"train\":\n",
    "        try:\n",
    "            optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "            optuna.delete_study(study_name=study_name, storage=storage_name)\n",
    "            print(f\"Deleted existing study: {study_name}\")\n",
    "        except KeyError:\n",
    "            print(f\"Study {study_name} does not exist, creating new one.\")\n",
    "        study = optuna.create_study(study_name=study_name, storage=storage_name, sampler=optuna.samplers.TPESampler(seed=42), direction='maximize')\n",
    "        # study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "        # study.optimize(objective, n_trials=10, n_jobs=10, show_progress_bar=True)\n",
    "        # Parallel(n_jobs=10)([delayed(optimize)(30) for _ in range(1)]) # 多线程并行运行\n",
    "        \n",
    "        study.optimize(objective, n_trials=20) # 单线程运行\n",
    "        logger.info(f\"Best parameters: {study.best_params}\")\n",
    "        logger.info(f\"Best score: {study.best_value}\")\n",
    "        file_handler.close()\n",
    "    elif mode == \"test\":\n",
    "        best_params = get_best_para_from_optuna(study_name=study_name, storage_name=storage_name)\n",
    "        evaluate_test_model(best_params)\n",
    "        # evaluate_test_model()\n",
    "    else:\n",
    "        NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    setup_seed(42)    \n",
    "    \n",
    "    main(mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.不同sample_weight下模型指标对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from tools.config import model_paths\n",
    "\n",
    "def load_model(model_name):\n",
    "    module_path = model_paths[model_name]\n",
    "    module = importlib.import_module(module_path)\n",
    "    return getattr(module, 'EGV_AttNet')\n",
    "\n",
    "# 加载模型\n",
    "EGV_AttNet_Class = load_model('EGV_AttNet')\n",
    "# EGV_AttNet_Class = load_model('simplecombine')\n",
    "# EGV_AttNet_Class = load_model('eGe_Net')\n",
    "# EGV_AttNet_Class = load_model('vgg_Net')\n",
    "\n",
    "# 存储不同sample_weight下的F1分数\n",
    "f1_scores = {}\n",
    "# 定义  sampl_weight 的范围\n",
    "sample_weights = np.linspace(0.1, 0.9, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test, \n",
    "                   sample_weight=0.5, epochs=50, patience=10, save_flag=False):\n",
    "    setup_seed(42)\n",
    "    best_f1 = 0.0  # 用于记录最佳 F1 分数\n",
    "    no_improvement_count = 0  # 记录未提升的轮数\n",
    "\n",
    "    # 将元组中的特征分别提取出来\n",
    "    X_train_eGe, X_train_VGGish = X_train\n",
    "    X_test_eGe, X_test_VGGish = X_test\n",
    "\n",
    "    # 确保输入是 NumPy 数组\n",
    "    X_train_eGe = np.asarray(X_train_eGe)\n",
    "    X_train_VGGish = np.asarray(X_train_VGGish)\n",
    "    X_test_eGe = np.asarray(X_test_eGe)\n",
    "    X_test_VGGish = np.asarray(X_test_VGGish)\n",
    "\n",
    "    # 将特征转换为张量\n",
    "    X_train_eGe = torch.tensor(X_train_eGe, dtype=torch.float32).to(device)\n",
    "    X_train_VGGish = torch.tensor(X_train_VGGish, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "    X_test_eGe = torch.tensor(X_test_eGe, dtype=torch.float32).to(device)\n",
    "    X_test_VGGish = torch.tensor(X_test_VGGish, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    groups = [list(range(i, min(i+6, len(X_train_eGe)))) for i in range(0, len(X_train_eGe), 6)] \n",
    "    \n",
    "    swanlab.init(\n",
    "        project=\"depression\",\n",
    "        workspace=\"2283118171\",\n",
    "        experiment_name=\"ATT 随机加权 optuna 6000\",\n",
    "        description=\"不进行标准化且去除音量增强。改为二元交叉熵损失BCEloss  设置小学习率和大epoch\",\n",
    "        # mode='disabled', # 调试模式\n",
    "        # callbacks=[wxwork_callback]\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        # # 初始化选中索引列表（保留所有原始样本）\n",
    "        # selected_indices = list(range(0, num_samples, 6))  # 所有组的第1个样本（原始数据）\n",
    "\n",
    "        # # 为每组随机选1个增强样本\n",
    "        # for group_start in range(0, num_samples, 6):\n",
    "        #     # 随机从该组的增强样本（1-5）中选1个\n",
    "        #     augment_index = random.randint(group_start + 1, group_start + 5)\n",
    "        #     selected_indices.append(augment_index)\n",
    "        \n",
    "        selected_indices = [random.choices(group, weights=[sample_weight] + [(1-sample_weight)/(len(group)-1)]*(len(group)-1), k=1)[0] for group in groups]\n",
    "\n",
    "        # 获取选中的数据\n",
    "        X_train_eGe_select = X_train_eGe[selected_indices]\n",
    "        X_train_VGGish_select = X_train_VGGish[selected_indices]\n",
    "        y_train_select = y_train[selected_indices]\n",
    "\n",
    "        \n",
    "        # 随机取\n",
    "\n",
    "        # outputs = model(X_train_eGe, X_train_VGGish).squeeze()  # 分别传入 eGeMAPS 和 VGGish 特征\n",
    "        outputs = model(X_train_eGe_select, X_train_VGGish_select).squeeze() \n",
    "        # loss = criterion(outputs, y_train)\n",
    "        loss = criterion(outputs, y_train_select)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        swanlab.log({\"train/loss\": loss}, step=epoch)\n",
    "\n",
    "        # 评估\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test_eGe, X_test_VGGish).squeeze()  # 分别传入 eGeMAPS 和 VGGish 特征\n",
    "            loss_test = criterion(val_outputs, y_test) \n",
    "            predictions_prob = val_outputs # 预测概率       \n",
    "            y_pred = (predictions_prob >= 0.5).int().cpu().numpy() # 预测标签\n",
    "            y_pred_prob = predictions_prob.cpu().numpy()         \n",
    "            evaluta_dic = evaluate_model(y_test.cpu().numpy(), y_pred, y_pred_prob)\n",
    "            swanlab.log({\"test/loss\": loss_test})\n",
    "            swanlab.log({\"test/f1\": evaluta_dic['f1']})\n",
    "            swanlab.log({\"test/auc\": evaluta_dic['roc_auc']})\n",
    "            swanlab.log({\"test/acc\": evaluta_dic['acc']})\n",
    "\n",
    "\n",
    "        # print(f\"{epoch} ==> loss: {loss:.4f}    f1; {evaluta_dic['f1']:.4f}\")\n",
    "        \n",
    "        # 早停检测\n",
    "        if evaluta_dic['f1'] > best_f1:\n",
    "            best_f1 = evaluta_dic['f1']\n",
    "            best_auc = evaluta_dic['roc_auc']\n",
    "            no_improvement_count = 0  # 重置计数器\n",
    "            swanlab.log({\"test/early_stop_epoch\": epoch})\n",
    "            swanlab.log({\"test/early_stop_f1\": best_f1})\n",
    "            swanlab.log({\"test/early_stop_auc\": best_auc})\n",
    "            if save_flag:\n",
    "                torch.save(model.state_dict(), \"../result/04modelPerformance/best_EgvAtt_model.pth\")  # 保存当前最佳模型\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        # 如果没有提升的轮数超过阈值，停止训练\n",
    "        if no_improvement_count >= patience:\n",
    "            # print(f\"Early stopping triggered. Best F1: {best_f1:.4f}\")\n",
    "            break\n",
    "    # torch.save(model.state_dict(), \"../result/04modelPerformance/best_EgvAtt_model.pth\")\n",
    "    swanlab.finish()\n",
    "    return best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_model(model_param=None):\n",
    "    setup_seed(42)\n",
    "    # lr = model_param['lr']\n",
    "    # transformed_feature_dim = model_param['transformed_feature_dim']\n",
    "    # weight_decay = model_param['weight_decay']\n",
    "    # conv_out_channels = model_param['conv_out_channels']\n",
    "    # epoch = model_param['epoch']\n",
    "    # patience = model_param['patience']\n",
    "    \n",
    "    # lr = model_param['lr']\n",
    "    # sample_weight = model_param['sample_weight']\n",
    "    # conv_out_channels = model_param['conv_out_channels']\n",
    "    # transformed_feature_dim = model_param['transformed_feature_dim']\n",
    "    # cls_dim = model_param['cls_dim']\n",
    "    lr = 0.00044408314231896245\n",
    "    conv_out_channels = 104\n",
    "    transformed_feature_dim = 250\n",
    "    cls_dim =207\n",
    "\n",
    "    # sample_weight = 0.6\n",
    "\n",
    "    weight_decay = 2e-4\n",
    "    expansion = 2\n",
    "    dropout = 0.1\n",
    "    resblock_kernel_size = 5\n",
    "    epoch = 2000\n",
    "    patience = 200\n",
    "       \n",
    "\n",
    "    # swanlab.config = {\n",
    "    #     \"lr\": lr,\n",
    "    #     \"transformed_feature_dim\": transformed_feature_dim,\n",
    "    #     \"weight_decay\": weight_decay,\n",
    "    #     \"epoch\": epoch,\n",
    "    #     \"patience\": patience,\n",
    "    #     \"conv_out_channels\": conv_out_channels,\n",
    "    #     \"eca_kernel_size\": eca_kernel_size,\n",
    "    #     \"resblock_kernel_size\": resblock_kernel_size,\n",
    "    # }\n",
    "    \n",
    "    # data_path = \"../data/modelBasicPerformance/dropout\"  # 没有做归一并且去除了音量增强\n",
    "    data_path = \"../data/modelBasicPerformance/\"  # 没有做归一并且去除了音量增强\n",
    "    X_train_eGe,X_test_eGe,X_val_eGe,X_train_VGGish,X_test_VGGish,X_val_VGGish,y_train,y_test,y_val = read_data(data_path)\n",
    "\n",
    "    X_train = (X_train_eGe[eGe_feature], X_train_VGGish)\n",
    "    X_test = (X_test_eGe[eGe_feature], X_test_VGGish)\n",
    "\n",
    "    assert feature_weights.index.tolist() == X_train[0].columns.tolist(), \"feature_weights order must be equal to the order of data columns\"\n",
    "    \n",
    "    # 遍历 sample_weight\n",
    "    for sample_weight in sample_weights:\n",
    "        print(f\"Testig with sample weight {sample_weight:.1f}\") \n",
    "        \n",
    "    model = EGV_AttNet_Class(input_dim_eGeMAPS=len(feature_weights), input_dim_VGGish=128, \n",
    "                       feature_weights=feature_weights, expansion=expansion, dropout=dropout,\n",
    "                       conv_out_channels=conv_out_channels,\n",
    "                       transformed_feature_dim=transformed_feature_dim, \n",
    "                       resblock_kernel_size=resblock_kernel_size, cls_dim=cls_dim).to(device)\n",
    "    \n",
    "    print(f\"Model: {EGV_AttNet_Class.__name__}\")\n",
    "\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # if criterion == \"bce\":\n",
    "    #     criterion = nn.BCELoss()\n",
    "    # elif criterion == 'focal':\n",
    "    #     focal_alpha = model_param['focal_alpha']\n",
    "    #     criterion = FocalLoss(alpha=focal_alpha)\n",
    "    # else:\n",
    "    #     NotImplementedError\n",
    "        \n",
    "    #训练并评估模型\n",
    "    f1 = train_evaluate(model, criterion, optimizer, X_train, y_train, X_test, y_test, \n",
    "                        sample_weight=sample_weight, epochs=epoch, patience=patience, \n",
    "                        save_flag=True)\n",
    "    \n",
    "    f1_scores[sample_weight] = f1\n",
    "\n",
    "    # print(f\"best f1: {f1}\")\n",
    "    def _plot_confusion_matrix(y_true, y_pred, classes, dataset_name, cmap=plt.cm.Blues):\n",
    "        plt.rcParams['font.family'] = 'Arial'\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, \n",
    "                    xticklabels=classes, yticklabels=classes)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "\n",
    "        plt.savefig(f'../result/04modelPerformance/01confusion_matrix_{dataset_name}.tif', dpi=300, bbox_inches=\"tight\")\n",
    "        plt.savefig(f'../result/04modelPerformance/01confusion_matrix_{dataset_name}.pdf', dpi=300, bbox_inches=\"tight\")\n",
    "        \n",
    "        # plt.show()\n",
    "\n",
    "    def _get_evaluate_dic(eGe, vgg, y_true, dataset_name, save_path):\n",
    "        outputs = model(eGe, vgg)\n",
    "        predictions_prob = outputs\n",
    "        y_pred = (predictions_prob >= 0.5).int().cpu().numpy()\n",
    "        y_pred_prob = predictions_prob.cpu().numpy()         \n",
    "        evaluate_dic = evaluate_model(y_true.cpu().numpy(), y_pred, y_pred_prob)\n",
    "        print(f\"{dataset_name} dataset:\\tF1: {evaluate_dic['f1']:.4f}\\tAUC: {evaluate_dic['roc_auc']:.4f}\\tACC: {evaluate_dic['acc']:.4f}\")\n",
    "        _plot_confusion_matrix(y_true.cpu().numpy(), y_pred, 2, dataset_name)\n",
    "\n",
    "        del evaluate_dic['fpr']\n",
    "        del evaluate_dic['tpr']\n",
    "        df = pd.DataFrame(evaluate_dic, index=[0])\n",
    "        \n",
    "        cols = ['f1','roc_auc','aupr','gmean','kappa','mcc','acc','npv','ppv','sensitivity','specificity']\n",
    "        df = df[cols]\n",
    "        df.to_excel(os.path.join(save_path, f\"{dataset_name}_evaluate.xlsx\"), index=False)\n",
    "\n",
    "    model.load_state_dict(torch.load(\"../result/04modelPerformance/best_EgvAtt_model.pth\"))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        X_test_eGe = np.asarray(X_test_eGe[eGe_feature])\n",
    "        X_val_eGe = np.asarray(X_val_eGe[eGe_feature])\n",
    "\n",
    "        X_test_VGGish = np.asarray(X_test_VGGish)\n",
    "        X_val_VGGish = np.asarray(X_val_VGGish)\n",
    "        # print(f\"y_test: {y_test}\")\n",
    "        # print(f\"y_val: {y_val}\")\n",
    "        # 将特征转换为张量\n",
    "        X_test_eGe = torch.tensor(X_test_eGe, dtype=torch.float32).to(device)\n",
    "        X_test_VGGish = torch.tensor(X_test_VGGish, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "        X_val_eGe = torch.tensor(X_val_eGe, dtype=torch.float32).to(device)\n",
    "        X_val_VGGish = torch.tensor(X_val_VGGish, dtype=torch.float32).to(device)\n",
    "        y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "        save_path = '../result/04modelPerformance/'\n",
    "        _get_evaluate_dic(X_test_eGe, X_test_VGGish, y_test, \"val\", save_path)\n",
    "        _get_evaluate_dic(X_val_eGe, X_val_VGGish, y_val, \"test\", save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Desktop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
